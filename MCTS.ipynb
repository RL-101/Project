{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiusoyeI/qfS33mXB6yN/L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fezilemahlangu/Reinforcement-Learning-Project/blob/master/MCTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "AK86zImAe8MW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75VyHPAnYk2e"
      },
      "outputs": [],
      "source": [
        "#https://towardsdatascience.com/deep-reinforcement-learning-and-monte-carlo-tree-search-with-connect-4-ba22a4713e7a\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install -y cmake\n",
        "!apt-get install -y build-essential autoconf libtool pkg-config\n",
        "!apt-get install flex bison libbz2-dev\n",
        "!pip install nle\n",
        "!pip install minihack\n",
        "!python -m minihack.scripts.env_list\n",
        "!pip install gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "ASHEtNDpe2BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "MxuFUSFZfOwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import nle\n",
        "import minihack\n",
        "from gym import spaces\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "yrhieh3LfQMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCTS"
      ],
      "metadata": {
        "id": "z57SZ_nlfUQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Node"
      ],
      "metadata": {
        "id": "37hV5k2_gQwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCTS():\n",
        "  '''\n",
        "  Class for MCTS node \n",
        "  '''\n",
        "  def __init__(self,state, parent,  parent_action):\n",
        "\n",
        "    self.state = state\n",
        "    self.parent = parent # parent node in MCTS\n",
        "    self.parent_action = parent_action # action that the parent took\n",
        "    self.children = [] #children of parent node \n",
        "    self.visit_count = 1 # keeps count of how many times node has been visited\n",
        "    self.rewards = defaultdict(int)\n",
        "  def expand(self,env):\n",
        "\n",
        "    '''\n",
        "    expand node and take unexplored action \n",
        "    '''\n",
        "    action = None #should be unexplored action \n",
        "\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    child_node = MCTS(obs,self,action) #obs should be state \n",
        "\n",
        "    self.children.append(child_node)\n",
        "\n",
        "    return child_node, env\n",
        "\n",
        "  def back_propagate(self,reward):\n",
        "    '''\n",
        "    Back propagate reward on all nodes from leaf to root and update visit count \n",
        "    '''\n",
        "    \n",
        "    self.visit_count += 1\n",
        "    self.rewards[reward] += 1\n",
        "\n",
        "    if self.parent:\n",
        "      self.parent.back_propagate(reward)\n",
        "\n",
        "  def unexplored_actions(self):\n",
        "    '''\n",
        "    returns unexplored actions \n",
        "    '''\n",
        "\n",
        "  def rollout_policy(self):\n",
        "    '''\n",
        "    rollout policy \n",
        "    '''\n",
        "\n",
        "  def tree_policy(self,env): #->fix \n",
        "    '''\n",
        "    keeps expanding tree until terminal node is reached \n",
        "    '''\n",
        "    curr = self\n",
        "    while not curr.state.done:\n",
        "      if len(self.children) < env.action_space.n:\n",
        "        self.expand(env)\n",
        "      else:\n",
        "        curr = curr.best_child(c_p = 0.1)\n",
        "\n",
        "    return curr, env\n",
        "\n",
        "\n",
        "\n",
        "  def best_action(self):\n",
        "    '''\n",
        "    find next best action \n",
        "    '''\n",
        "\n",
        "\n",
        "  \n",
        "  def best_child(self,c_p):\n",
        "    '''\n",
        "    finds best child using UCT\n",
        "    '''\n",
        "\n",
        "    ns = self.visit_count #visit count of parent\n",
        "\n",
        "    ni = [c.visit_count for c in self.children] #visit count of chilren \n",
        "\n",
        "    q = 0\n",
        "    for r in self.rewards:\n",
        "      q += r*self.rewards[r]\n",
        "\n",
        "    first_term = q / ni\n",
        "\n",
        "    second_term = c_p * np.sqrt((2*np.log(ns))/ni) \n",
        "\n",
        "    UCB1 = first_term + second_term \n",
        "\n",
        "    best_child = np.argmax(UCB1)\n",
        "\n",
        "    return self.children[best_child]\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "QstfHIxqfU-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "DNk7Sj411Nmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MiniHack-Quest-Hard-v0\")\n",
        "\n",
        "init_state = env.reset()"
      ],
      "metadata": {
        "id": "7kru-NQj1NG7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}