{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3iY5RUSa3xxKzMsb/+3Oq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fezilemahlangu/Reinforcement-Learning-Project/blob/master/A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "738iuUMB6EKo",
        "outputId": "9d25fff1-887f-4740-8e57-571d325c3fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,217 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,467 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,257 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,181 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,117 kB]\n",
            "Fetched 11.9 MB in 6s (1,959 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  automake autotools-dev file libmagic-mgc libmagic1 libsigsegv2 m4\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc gettext libtool-doc gcj-jdk\n",
            "  m4-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autotools-dev file libmagic-mgc libmagic1 libsigsegv2\n",
            "  libtool m4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,551 kB of archives.\n",
            "After this operation, 10.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Fetched 1,551 kB in 0s (4,744 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../1-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../2-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "Preparing to unpack .../3-libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../4-m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../5-autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../6-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../7-automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../8-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libbison-dev libfl-dev libfl2\n",
            "Suggested packages:\n",
            "  bison-doc flex-doc\n",
            "The following NEW packages will be installed:\n",
            "  bison flex libbison-dev libfl-dev libfl2\n",
            "0 upgraded, 5 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 938 kB of archives.\n",
            "After this operation, 2,925 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 flex amd64 2.6.4-6 [316 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libbison-dev amd64 2:3.0.4.dfsg-1build1 [339 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 bison amd64 2:3.0.4.dfsg-1build1 [266 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfl2 amd64 2.6.4-6 [11.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfl-dev amd64 2.6.4-6 [6,320 B]\n",
            "Fetched 938 kB in 0s (2,451 kB/s)\n",
            "Selecting previously unselected package flex.\n",
            "(Reading database ... 124284 files and directories currently installed.)\n",
            "Preparing to unpack .../flex_2.6.4-6_amd64.deb ...\n",
            "Unpacking flex (2.6.4-6) ...\n",
            "Selecting previously unselected package libbison-dev:amd64.\n",
            "Preparing to unpack .../libbison-dev_2%3a3.0.4.dfsg-1build1_amd64.deb ...\n",
            "Unpacking libbison-dev:amd64 (2:3.0.4.dfsg-1build1) ...\n",
            "Selecting previously unselected package bison.\n",
            "Preparing to unpack .../bison_2%3a3.0.4.dfsg-1build1_amd64.deb ...\n",
            "Unpacking bison (2:3.0.4.dfsg-1build1) ...\n",
            "Selecting previously unselected package libfl2:amd64.\n",
            "Preparing to unpack .../libfl2_2.6.4-6_amd64.deb ...\n",
            "Unpacking libfl2:amd64 (2.6.4-6) ...\n",
            "Selecting previously unselected package libfl-dev:amd64.\n",
            "Preparing to unpack .../libfl-dev_2.6.4-6_amd64.deb ...\n",
            "Unpacking libfl-dev:amd64 (2.6.4-6) ...\n",
            "Setting up flex (2.6.4-6) ...\n",
            "Setting up libbison-dev:amd64 (2:3.0.4.dfsg-1build1) ...\n",
            "Setting up libfl2:amd64 (2.6.4-6) ...\n",
            "Setting up bison (2:3.0.4.dfsg-1build1) ...\n",
            "update-alternatives: using /usr/bin/bison.yacc to provide /usr/bin/yacc (yacc) in auto mode\n",
            "Setting up libfl-dev:amd64 (2.6.4-6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nle\n",
            "  Downloading nle-0.8.1.tar.gz (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 4.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.15 in /usr/local/lib/python3.7/dist-packages (from nle) (0.25.2)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nle) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (3.9.0)\n",
            "Building wheels for collected packages: nle\n",
            "  Building wheel for nle (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nle: filename=nle-0.8.1-cp37-cp37m-linux_x86_64.whl size=2883121 sha256=44a7aecebd884fac2fc560e3365a3a9ad2d9677de71c1475cecddd71475134a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/43/b7/00eec64b2f64dc45883624bcb42a969645c86814ea751c6299\n",
            "Successfully built nle\n",
            "Installing collected packages: pybind11, nle\n",
            "Successfully installed nle-0.8.1 pybind11-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting minihack\n",
            "  Downloading minihack-0.1.3.tar.gz (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 4.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nle>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from minihack) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from minihack) (1.21.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from minihack) (0.25.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from nle>=0.8.0->minihack) (2.10.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (3.9.0)\n",
            "Building wheels for collected packages: minihack\n",
            "  Building wheel for minihack (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minihack: filename=minihack-0.1.3-py3-none-any.whl size=261792 sha256=57c940a24dfe10f8f8a9798d1bb945b3369ca243d4d8455048d007e34b44dacd\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/b0/50/bb8c09fe5befa92b343025c26d614c5fa312f1edb432cc9580\n",
            "Successfully built minihack\n",
            "Installing collected packages: minihack\n",
            "Successfully installed minihack-0.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.7 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=b9d8410b800e582469e2269183fc3ffa2e3defab0bad8ea93f690363b261243a\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install -y cmake\n",
        "!apt-get install -y build-essential autoconf libtool pkg-config\n",
        "!apt-get install flex bison libbz2-dev\n",
        "!pip install nle\n",
        "!pip install minihack\n",
        "# !python -m minihack.scripts.env_list\n",
        "!pip install gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# ACTOR CRITIC"
      ],
      "metadata": {
        "id": "fJSdJ2Mb_ic2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import minihack\n",
        "from minihack import reward_manager\n",
        "import numpy as np\n",
        "from minihack import RewardManager\n",
        "from gym import spaces\n",
        "from nle import nethack\n",
        "from numpy.lib.function_base import select\n",
        "import torch  \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nvcPRYsX6Iex"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Modified the A2C method from https://github.com/raillab/a2c\n",
        "This A2C has LSTM\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"\n",
        "    Flatten a multi dimensional output from the Conv2D to a single dimension\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, glyph_shape, num_actions, enable_lstm, crop_dims=10):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.flatten = Flatten()\n",
        "        if enable_lstm:\n",
        "            self.lstm = nn.LSTMCell(256, 256)\n",
        "        self.linear = nn.Linear(1152, 256)\n",
        "        self.actor = nn.Linear(256, num_actions)\n",
        "        self.critic = nn.Linear(256, 1)\n",
        "\n",
        "        for m in self.features:\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.orthogonal_(m.weight, nn.init.calculate_gain('relu'))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "        nn.init.orthogonal_(self.linear.weight)\n",
        "        nn.init.constant_(self.linear.bias, 0.0)\n",
        "\n",
        "        nn.init.orthogonal_(self.critic.weight)\n",
        "        nn.init.constant_(self.critic.bias, 0.0)\n",
        "\n",
        "        nn.init.orthogonal_(self.actor.weight, 0.01)\n",
        "        nn.init.constant_(self.actor.bias, 0.0)\n",
        "\n",
        "    def forward(self, x_glyphs, hx, cx, enable_lstm):\n",
        "        x_glyphs = x_glyphs.unsqueeze(0)\n",
        "        x_glyphs = self.features(x_glyphs)\n",
        "        x_glyphs = self.flatten(x_glyphs)\n",
        "        x_glyphs = self.linear(x_glyphs)\n",
        "        x_glyphs = F.relu(x_glyphs)\n",
        "        if enable_lstm:\n",
        "            hx, cx = self.lstm(x_glyphs, (hx, cx))\n",
        "            x = hx\n",
        "        else:\n",
        "            x = x_glyphs\n",
        "        return Categorical(logits=self.actor(x)), self.critic(x), hx, cx"
      ],
      "metadata": {
        "id": "tr84jHYn6SsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# REINFORCE"
      ],
      "metadata": {
        "id": "RcA08ezq_e8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import minihack\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Func\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "from collections import deque\n",
        "from nle import nethack\n",
        "from minihack import RewardManager\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class SimplePolicy(nn.Module):\n",
        "    def __init__(self, s_size, h_size, a_size, learning_rate=0.001):\n",
        "        super(SimplePolicy, self).__init__()\n",
        "        self.linear1 = nn.Linear(s_size, h_size)\n",
        "        self.linear2 = nn.Linear(h_size, a_size)\n",
        "        self.loss_fn=nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"before: \",x.shape)\n",
        "        x = torch.flatten(x)\n",
        "        x = torch.reshape(x, (1,x.shape[0]))\n",
        "        #print(\"after: \",x.shape)\n",
        "        #normalize tensors\n",
        "        x = torch.nn.functional.normalize(x, p=2.0, dim=1, eps=1e-12, out=None)\n",
        "        func = Func.relu(self.linear1(x))\n",
        "        #print(\"func: \",func)\n",
        "        func = Func.softmax(self.linear2(func), dim=1)\n",
        "        return func\n",
        "\n",
        "\n",
        "class StateValueNetwork(nn.Module):\n",
        "    # Takes in state\n",
        "    def __init__(self, s_size=4, h_size=16, learning_rate=0.001):\n",
        "        super(StateValueNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(s_size, h_size)\n",
        "        self.linear2 = nn.Linear(h_size, 1)\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #input layer\n",
        "        x = torch.flatten(x)\n",
        "        x = torch.reshape(x, (1,x.shape[0]))\n",
        "        f = self.linear1(x)\n",
        "        #activiation relu\n",
        "        f = Func.relu(f)\n",
        "        #get state value\n",
        "        state_value = self.linear2(f)\n",
        "        return state_value\n",
        "\n",
        "def compute_returns_naive_baseline(rewards, gamma):\n",
        "    returns = []\n",
        "    #calculates the return values\n",
        "    for t in range(len(rewards)):\n",
        "        Gt = 0\n",
        "        for r in rewards[t:]:\n",
        "            Gt = Gt * gamma + r\n",
        "        returns.append(Gt)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "    returns = (returns - returns.mean()) / (\n",
        "        returns.std())\n",
        "    return returns\n",
        "\n",
        "def learning(states ,scores, state_model, policy_model, lProbs, env, gamma):\n",
        "    returns = compute_returns_naive_baseline(scores, gamma)\n",
        "    #env.render()\n",
        "    #this section calculates the state values\n",
        "    #calculate MSE loss\n",
        "    stateValues = []\n",
        "    for i in states:\n",
        "        stateValues.append(state_model.forward(Variable(i)))\n",
        "    stateValues = torch.stack(stateValues).squeeze()\n",
        "    valLoss = Func.mse_loss(stateValues, returns)\n",
        "    #backpropagate\n",
        "    state_model.optimizer.zero_grad()\n",
        "    valLoss.backward()\n",
        "    state_model.optimizer.step()\n",
        "    deltas = []\n",
        "    for gt, val in zip(returns, stateValues):\n",
        "        deltas.append(gt-val)\n",
        "    deltas = torch.tensor(deltas).to(device)\n",
        "    #this section is where we calculate the policy gradient\n",
        "    #this section is from https://gist.github.com/cyoon1729/3920da556f992909ace8516e2f321a7c#file-reinforce_update-py\n",
        "    policyGrad = []\n",
        "    #training policy\n",
        "    for logProb, Dt in zip(lProbs, deltas):\n",
        "        policyGrad.append(-logProb * Dt)\n",
        "    policy_model.optimizer.zero_grad()\n",
        "    policyGrad = torch.stack(policyGrad).sum()\n",
        "    #backpropagate\n",
        "    policyGrad.backward()\n",
        "    policy_model.optimizer.step()\n",
        "\n",
        "def reinforce_naive_baseline(env, policy_model, state_model, seed,\n",
        "                             number_episodes,\n",
        "                             max_episode_length,\n",
        "                             gamma, verbose=True):\n",
        "    global hyper_params\n",
        "    # set random seeds (for reproducibility)\n",
        "    torch.manual_seed(hyper_params['seed'])\n",
        "    torch.cuda.manual_seed_all(hyper_params['seed'])\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "    env.seed(hyper_params['seed'])\n",
        "    policy = []\n",
        "    numsteps = []\n",
        "    avgNumsteps = []\n",
        "    allRewards = []\n",
        "    for episode in range(number_episodes):\n",
        "        state = env.reset()['glyphs_crop']\n",
        "        lProbs = []\n",
        "        scores = []\n",
        "        states = []\n",
        "        for steps in range(max_episode_length):\n",
        "            #env.render()\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            #this section is from https://gist.github.com/cyoon1729/bc41d466b868ea10e794a7c04321ff3b#file-reinforce_model-py\n",
        "            probs = policy_model.forward(Variable(state))\n",
        "            action = np.random.choice(env.action_space.n)\n",
        "            lprob = torch.log(probs.squeeze(0)[action])\n",
        "            nextState, score, done, _ = env.step(action)\n",
        "            lProbs.append(lprob)\n",
        "            scores.append(score)\n",
        "            states.append(state)\n",
        "            if steps%100==0:\n",
        "                p = 0\n",
        "                #learning(states, scores, state_model, policy_model, lProbs, env, gamma)\n",
        "            if done:\n",
        "                learning(states, scores, state_model, policy_model, lProbs, env, gamma)\n",
        "                numsteps.append(steps)\n",
        "                avgNumsteps.append(np.mean(numsteps[-10:]))\n",
        "                allRewards.append(np.sum(scores))\n",
        "                if episode % 1 == 0:\n",
        "                    print(\"Reinforce with baseline -> episode: {}, total reward: {}, average_reward: {}, length: {}\".format(episode,np.round(\n",
        "                                                                                                                  np.sum(\n",
        "                                                                                                                      scores),\n",
        "                                                                                                                  decimals=3),\n",
        "                                                                                                              np.round(\n",
        "                                                                                                                  np.mean(\n",
        "                                                                                                                      allRewards[\n",
        "                                                                                                                      -10:]),\n",
        "                                                                                                                  decimals=3),\n",
        "                                                                                                      steps))\n",
        "                break\n",
        "            state = nextState['glyphs_crop']\n",
        "    env.close()\n",
        "    return policy, allRewards\n",
        "import cv2\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "class ExploreEvent(minihack.reward_manager.Event):\n",
        "    def __init__(self, reward: float, repeatable: bool, terminal_required: bool, terminal_sufficient: bool):\n",
        "        super().__init__(reward, repeatable, terminal_required, terminal_sufficient)\n",
        "\n",
        "    def check(self, env, previous_observation, action, observation) -> float:\n",
        "        # blank spots are 32\n",
        "        # agent is 64\n",
        "        # agent spawn point is 60\n",
        "        # pathways are 35\n",
        "        # obs[1] is the char observation \n",
        "        # print(\"+++++++++++++++++++\\nobs = \\n++++++++++++++++++++++\\n\", observation[1])\n",
        "        current = sum(np.count_nonzero(i == 35) for i in observation[1])\n",
        "        current += sum(np.count_nonzero(i == 60) for i in observation[1])\n",
        "        prev = sum(np.count_nonzero(i == 35) for i in previous_observation[1])\n",
        "        prev += sum(np.count_nonzero(i == 60) for i in previous_observation[1])\n",
        "        if current > prev:\n",
        "            return self.reward\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "class RenderRGB(gym.Wrapper):\n",
        "    def __init__(self, env, key_name=\"pixel\"):\n",
        "        super().__init__(env)\n",
        "        self.last_pixels = None\n",
        "        self.viewer = None\n",
        "        self.key_name = key_name\n",
        "\n",
        "        render_modes = env.metadata['render.modes']\n",
        "        render_modes.append(\"rgb_array\")\n",
        "        env.metadata['render.modes'] = render_modes\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.last_pixels = obs[self.key_name]\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def render(self, mode=\"human\", **kwargs):\n",
        "        img = self.last_pixels\n",
        "\n",
        "        # Hacky but works\n",
        "        if mode != \"human\":\n",
        "            return img\n",
        "        else:\n",
        "            from gym.envs.classic_control import rendering\n",
        "\n",
        "            if self.viewer is None:\n",
        "                self.viewer = rendering.SimpleImageViewer()\n",
        "            self.viewer.imshow(img)\n",
        "            return self.viewer.isopen\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        self.last_pixels = obs[self.key_name]\n",
        "        return obs\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer is not None:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None\n",
        "\n",
        "def create_env():\n",
        "    global hyper_params\n",
        "    # ACTIONS define the actions allowed by the agent\n",
        "    MOVE_ACTIONS = tuple(nethack.CompassDirection)\n",
        "    NAVIGATE_ACTIONS = MOVE_ACTIONS + (\n",
        "        nethack.Command.OPEN,   # Not sure if needed\n",
        "        nethack.Command.PICKUP, \n",
        "        nethack.Command.WEAR,   \n",
        "        nethack.Command.WIELD,  \n",
        "        nethack.Command.QUAFF,\n",
        "        nethack.Command.INVOKE,\n",
        "        nethack.Command.ZAP,\n",
        "        nethack.Command.SWAP,   # Not sure if needed\n",
        "\n",
        "        # Might need more? All actions and descriptions found here\n",
        "        # https://minihack.readthedocs.io/en/latest/getting-started/action_spaces.html\n",
        "    )\n",
        "    pixel_obs = \"pixel_crop\"\n",
        "\n",
        "    reward_manager = RewardManager()\n",
        "    reward_manager.add_kill_event(\"minotaur\", reward=1, terminal_required=False)\n",
        "    strings = list()\n",
        "    strings.append(\"The door opens.\")\n",
        "    reward_manager.add_message_event(strings, reward=1, terminal_required=True)\n",
        "\n",
        "    strings = list()\n",
        "    strings.append(\"It's solid stone.\")\n",
        "    reward_manager.add_message_event(\"It's solid stone.\", reward=-0.75, terminal_required=False, repeatable=True)\n",
        "\n",
        "    reward_manager.add_event(ExploreEvent(0.5, True, True, False))\n",
        "    \n",
        "    # Create env with modified actions\n",
        "    # Probably can limit the observations as well\n",
        "    env = gym.make(\n",
        "        hyper_params[\"env-name\"],\n",
        "        observation_keys=(\"glyphs_crop\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", pixel_obs),\n",
        "        actions=NAVIGATE_ACTIONS,\n",
        "        reward_lose=-1,\n",
        "        reward_win=1,\n",
        "        savedir=\"./games\",\n",
        "        reward_manager=reward_manager\n",
        "    )\n",
        "\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "    env = RenderRGB(env, pixel_obs)\n",
        "    # env = gym.wrappers.Monitor(env, \"recordings\", force=True)\n",
        "\n",
        "    return env\n",
        "\n",
        "def run_reinforce():\n",
        "    global hyper_params\n",
        "    env = create_env()\n",
        "    print(\"number of actions: \",env.action_space)\n",
        "    #print(env.observation_space['glyphs'])\n",
        "    #deimension of game space\n",
        "    size = 9 * 9\n",
        "    hSize = round(size/2)\n",
        "    num_epi = 50\n",
        "    policy_model = SimplePolicy(s_size=size, h_size=size, a_size=env.action_space.n,learning_rate=hyper_params['learning-rate']).to(device)\n",
        "    stateval_model = StateValueNetwork(s_size=size, h_size=size,learning_rate=hyper_params['learning-rate']).to(device)\n",
        "    policy, scores = reinforce_naive_baseline(env=env, policy_model=policy_model, state_model=stateval_model, seed=42,\n",
        "                               number_episodes=num_epi,\n",
        "                               max_episode_length=hyper_params['num-steps'],\n",
        "                               gamma=hyper_params['discount-factor'],\n",
        "                               verbose=True)\n",
        "    # Plot learning curve\n",
        "    plt.plot(scores,'o')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Average reward')\n",
        "    plt.title('Average reward per episode')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    hyper_params = {\n",
        "        \"seed\": 42,  # which seed to use\n",
        "        \"env-name\": \"MiniHack-Quest-Hard-v0\",  # name of the game\n",
        "        \"learning-rate\": 1e-2,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(10000),  # total number of steps to run the environment for\n",
        "        \"print-freq\": 25, # number of iterations between each print out\n",
        "        \"save-freq\": 500, # number of iterations between each model save\n",
        "    }\n",
        "    run_reinforce()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv6bXdb9_gX7",
        "outputId": "3ff8030c-3830-4b9a-b54a-8d6dbf95229d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of actions:  Discrete(16)\n",
            "Reinforce with baseline -> episode: 0, total reward: -700.48, average_reward: -700.48, length: 999\n",
            "Reinforce with baseline -> episode: 1, total reward: -703.48, average_reward: -701.98, length: 999\n",
            "Reinforce with baseline -> episode: 2, total reward: -727.19, average_reward: -710.383, length: 999\n",
            "Reinforce with baseline -> episode: 3, total reward: -715.83, average_reward: -711.745, length: 999\n",
            "Reinforce with baseline -> episode: 4, total reward: -725.91, average_reward: -714.578, length: 999\n",
            "Reinforce with baseline -> episode: 5, total reward: -715.48, average_reward: -714.728, length: 999\n",
            "Reinforce with baseline -> episode: 6, total reward: -707.11, average_reward: -713.64, length: 999\n",
            "Reinforce with baseline -> episode: 7, total reward: -698.48, average_reward: -711.745, length: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "3vL267zn_v64"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}