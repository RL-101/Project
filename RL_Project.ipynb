{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZjhnWIQoREtx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fezilemahlangu/Reinforcement-Learning-Project/blob/master/RL_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODOS\n",
        "\n",
        "* learn how to play replays using Jettyplay because nle dashboard no longer exists."
      ],
      "metadata": {
        "id": "E3Jke66CfQVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies (run then restart):"
      ],
      "metadata": {
        "id": "ZjhnWIQoREtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing nle dependencies:"
      ],
      "metadata": {
        "id": "3WYE2PmaQyuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update\n",
        "!apt install -y cmake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCX29m5hIYWZ",
        "outputId": "5f534c83-2d46-4abb-e192-28a8d14add17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [1 InRelease 14.2 kB/88.7 kB 16%] [Connec\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 57.6 kB/88.\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [1 InRelease 77.9 kB/88.\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\u001b[0m\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (185.125.190.36\u001b[0m\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [Wait\u001b[0m\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\u001b[33m\r                                                                               \r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                         \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\u001b[33m\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers]\u001b[0m\r                                                                         \rHit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,020 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,188 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,551 kB]\n",
            "Get:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,164 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,452 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,109 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,329 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,230 kB]\n",
            "Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Get:23 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [50.8 kB]\n",
            "Fetched 16.5 MB in 4s (4,064 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y build-essential autoconf libtool pkg-config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQrHwygzM3JE",
        "outputId": "c5eae858-eb36-4c2c-8a2a-3adf713e0111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  automake autotools-dev file libmagic-mgc libmagic1 libsigsegv2 m4\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc gettext libtool-doc gcj-jdk\n",
            "  m4-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autotools-dev file libmagic-mgc libmagic1 libsigsegv2\n",
            "  libtool m4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 1,551 kB of archives.\n",
            "After this operation, 10.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Fetched 1,551 kB in 1s (1,625 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 123934 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../1-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../2-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "Preparing to unpack .../3-libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../4-m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../5-autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../6-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../7-automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../8-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install flex bison libbz2-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6lGiaSHNgDF",
        "outputId": "ce73ff38-fcf3-4dda-b6a5-02d4cff2a0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libbison-dev libfl-dev libfl2\n",
            "Suggested packages:\n",
            "  bison-doc flex-doc\n",
            "The following NEW packages will be installed:\n",
            "  bison flex libbison-dev libfl-dev libfl2\n",
            "0 upgraded, 5 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 938 kB of archives.\n",
            "After this operation, 2,925 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 flex amd64 2.6.4-6 [316 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libbison-dev amd64 2:3.0.4.dfsg-1build1 [339 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 bison amd64 2:3.0.4.dfsg-1build1 [266 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfl2 amd64 2.6.4-6 [11.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfl-dev amd64 2.6.4-6 [6,320 B]\n",
            "Fetched 938 kB in 1s (1,078 kB/s)\n",
            "Selecting previously unselected package flex.\n",
            "(Reading database ... 124276 files and directories currently installed.)\n",
            "Preparing to unpack .../flex_2.6.4-6_amd64.deb ...\n",
            "Unpacking flex (2.6.4-6) ...\n",
            "Selecting previously unselected package libbison-dev:amd64.\n",
            "Preparing to unpack .../libbison-dev_2%3a3.0.4.dfsg-1build1_amd64.deb ...\n",
            "Unpacking libbison-dev:amd64 (2:3.0.4.dfsg-1build1) ...\n",
            "Selecting previously unselected package bison.\n",
            "Preparing to unpack .../bison_2%3a3.0.4.dfsg-1build1_amd64.deb ...\n",
            "Unpacking bison (2:3.0.4.dfsg-1build1) ...\n",
            "Selecting previously unselected package libfl2:amd64.\n",
            "Preparing to unpack .../libfl2_2.6.4-6_amd64.deb ...\n",
            "Unpacking libfl2:amd64 (2.6.4-6) ...\n",
            "Selecting previously unselected package libfl-dev:amd64.\n",
            "Preparing to unpack .../libfl-dev_2.6.4-6_amd64.deb ...\n",
            "Unpacking libfl-dev:amd64 (2.6.4-6) ...\n",
            "Setting up flex (2.6.4-6) ...\n",
            "Setting up libbison-dev:amd64 (2:3.0.4.dfsg-1build1) ...\n",
            "Setting up libfl2:amd64 (2.6.4-6) ...\n",
            "Setting up bison (2:3.0.4.dfsg-1build1) ...\n",
            "update-alternatives: using /usr/bin/bison.yacc to provide /usr/bin/yacc (yacc) in auto mode\n",
            "Setting up libfl-dev:amd64 (2.6.4-6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install minihack (nle is a dependency):"
      ],
      "metadata": {
        "id": "wo37smBwQjGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmV127SV91bC",
        "outputId": "f3ab6d5b-10f4-47f7-867d-df54abd403ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nle\n",
            "  Downloading nle-0.8.1.tar.gz (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 5.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.0-py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: gym>=0.15 in /usr/local/lib/python3.7/dist-packages (from nle) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nle) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (5.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (0.0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (3.9.0)\n",
            "Building wheels for collected packages: nle\n",
            "  Building wheel for nle (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nle: filename=nle-0.8.1-cp37-cp37m-linux_x86_64.whl size=2883127 sha256=690590c1319462a000894745a3089b43202ea9b54b9b6f15a48344a8fee40f09\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/43/b7/00eec64b2f64dc45883624bcb42a969645c86814ea751c6299\n",
            "Successfully built nle\n",
            "Installing collected packages: pybind11, nle\n",
            "Successfully installed nle-0.8.1 pybind11-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting minihack\n",
            "  Downloading minihack-0.1.3.tar.gz (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 5.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nle>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from minihack) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from minihack) (1.21.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from minihack) (0.25.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from nle>=0.8.0->minihack) (2.10.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (3.9.0)\n",
            "Building wheels for collected packages: minihack\n",
            "  Building wheel for minihack (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minihack: filename=minihack-0.1.3-py3-none-any.whl size=261792 sha256=e6c13a115559cb80a700226f20a8944fdfecd25441ce1f43c31e331960d08ed3\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/b0/50/bb8c09fe5befa92b343025c26d614c5fa312f1edb432cc9580\n",
            "Successfully built minihack\n",
            "Installing collected packages: minihack\n",
            "Successfully installed minihack-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install nle\n",
        "!pip install minihack"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if installation worked:"
      ],
      "metadata": {
        "id": "HHJvu_K7QdBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m minihack.scripts.env_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTwqD2DNO5fv",
        "outputId": "2215e57a-22e6-435b-aa5c-30c958c53b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:422: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
            "  \"The `registry.all` method is deprecated. Please use `registry.values` instead.\"\n",
            "MiniHack-Room-5x5-v0\n",
            "MiniHack-Room-Random-5x5-v0\n",
            "MiniHack-Room-Dark-5x5-v0\n",
            "MiniHack-Room-Monster-5x5-v0\n",
            "MiniHack-Room-Trap-5x5-v0\n",
            "MiniHack-Room-Ultimate-5x5-v0\n",
            "MiniHack-Room-15x15-v0\n",
            "MiniHack-Room-Random-15x15-v0\n",
            "MiniHack-Room-Dark-15x15-v0\n",
            "MiniHack-Room-Monster-15x15-v0\n",
            "MiniHack-Room-Trap-15x15-v0\n",
            "MiniHack-Room-Ultimate-15x15-v0\n",
            "MiniHack-Corridor-R2-v0\n",
            "MiniHack-Corridor-R3-v0\n",
            "MiniHack-Corridor-R5-v0\n",
            "MiniHack-KeyRoom-Fixed-S5-v0\n",
            "MiniHack-KeyRoom-S5-v0\n",
            "MiniHack-KeyRoom-S15-v0\n",
            "MiniHack-KeyRoom-Dark-S5-v0\n",
            "MiniHack-KeyRoom-Dark-S15-v0\n",
            "MiniHack-MazeWalk-9x9-v0\n",
            "MiniHack-MazeWalk-Mapped-9x9-v0\n",
            "MiniHack-MazeWalk-15x15-v0\n",
            "MiniHack-MazeWalk-Mapped-15x15-v0\n",
            "MiniHack-MazeWalk-45x19-v0\n",
            "MiniHack-MazeWalk-Mapped-45x19-v0\n",
            "MiniHack-CorridorBattle-v0\n",
            "MiniHack-CorridorBattle-Dark-v0\n",
            "MiniHack-MultiRoom-N2-v0\n",
            "MiniHack-MultiRoom-N4-v0\n",
            "MiniHack-MultiRoom-N6-v0\n",
            "MiniHack-MultiRoom-N2-Locked-v0\n",
            "MiniHack-MultiRoom-N4-Locked-v0\n",
            "MiniHack-MultiRoom-N6-Locked-v0\n",
            "MiniHack-MultiRoom-N2-Lava-v0\n",
            "MiniHack-MultiRoom-N4-Lava-v0\n",
            "MiniHack-MultiRoom-N6-Lava-v0\n",
            "MiniHack-MultiRoom-N2-Monster-v0\n",
            "MiniHack-MultiRoom-N4-Monster-v0\n",
            "MiniHack-MultiRoom-N6-Monster-v0\n",
            "MiniHack-MultiRoom-N2-Extreme-v0\n",
            "MiniHack-MultiRoom-N4-Extreme-v0\n",
            "MiniHack-MultiRoom-N6-Extreme-v0\n",
            "MiniHack-LavaCrossingS9N1-v0\n",
            "MiniHack-LavaCrossingS9N2-v0\n",
            "MiniHack-LavaCrossingS9N3-v0\n",
            "MiniHack-LavaCrossingS11N5-v0\n",
            "MiniHack-SimpleCrossingS9N1-v0\n",
            "MiniHack-SimpleCrossingS9N2-v0\n",
            "MiniHack-SimpleCrossingS9N3-v0\n",
            "MiniHack-SimpleCrossingS11N5-v0\n",
            "MiniHack-Memento-Short-F2-v0\n",
            "MiniHack-Memento-F2-v0\n",
            "MiniHack-Memento-F4-v0\n",
            "MiniHack-Boxoban-Unfiltered-v0\n",
            "MiniHack-Boxoban-Medium-v0\n",
            "MiniHack-Boxoban-Hard-v0\n",
            "MiniHack-River-v0\n",
            "MiniHack-River-Monster-v0\n",
            "MiniHack-River-Lava-v0\n",
            "MiniHack-River-MonsterLava-v0\n",
            "MiniHack-River-Narrow-v0\n",
            "MiniHack-HideNSeek-Mapped-v0\n",
            "MiniHack-HideNSeek-v0\n",
            "MiniHack-HideNSeek-Lava-v0\n",
            "MiniHack-HideNSeek-Big-v0\n",
            "MiniHack-Labyrinth-Big-v0\n",
            "MiniHack-Labyrinth-Small-v0\n",
            "MiniHack-ExploreMaze-Easy-v0\n",
            "MiniHack-ExploreMaze-Hard-v0\n",
            "MiniHack-ExploreMaze-Easy-Mapped-v0\n",
            "MiniHack-ExploreMaze-Hard-Mapped-v0\n",
            "MiniHack-Eat-v0\n",
            "MiniHack-Pray-v0\n",
            "MiniHack-Sink-v0\n",
            "MiniHack-Wield-v0\n",
            "MiniHack-Wear-v0\n",
            "MiniHack-PutOn-v0\n",
            "MiniHack-Zap-v0\n",
            "MiniHack-Read-v0\n",
            "MiniHack-Eat-Fixed-v0\n",
            "MiniHack-Pray-Fixed-v0\n",
            "MiniHack-Sink-Fixed-v0\n",
            "MiniHack-Wield-Fixed-v0\n",
            "MiniHack-Wear-Fixed-v0\n",
            "MiniHack-PutOn-Fixed-v0\n",
            "MiniHack-Zap-Fixed-v0\n",
            "MiniHack-Read-Fixed-v0\n",
            "MiniHack-Eat-Distr-v0\n",
            "MiniHack-Pray-Distr-v0\n",
            "MiniHack-Sink-Distr-v0\n",
            "MiniHack-Wield-Distr-v0\n",
            "MiniHack-Wear-Distr-v0\n",
            "MiniHack-PutOn-Distr-v0\n",
            "MiniHack-Zap-Distr-v0\n",
            "MiniHack-Read-Distr-v0\n",
            "MiniHack-ClosedDoor-v0\n",
            "MiniHack-LockedDoor-v0\n",
            "MiniHack-LockedDoor-Fixed-v0\n",
            "MiniHack-WoD-Easy-v0\n",
            "MiniHack-WoD-Medium-v0\n",
            "MiniHack-WoD-Hard-v0\n",
            "MiniHack-WoD-Pro-v0\n",
            "MiniHack-Levitate-Boots-v0\n",
            "MiniHack-Levitate-Ring-v0\n",
            "MiniHack-Levitate-Potion-v0\n",
            "MiniHack-Levitate-Random-v0\n",
            "MiniHack-Levitate-Boots-Fixed-v0\n",
            "MiniHack-Levitate-Ring-Fixed-v0\n",
            "MiniHack-Levitate-Potion-Fixed-v0\n",
            "MiniHack-Freeze-Wand-v0\n",
            "MiniHack-Freeze-Horn-v0\n",
            "MiniHack-Freeze-Random-v0\n",
            "MiniHack-Freeze-Lava-v0\n",
            "MiniHack-LavaCross-Levitate-Potion-Pickup-v0\n",
            "MiniHack-LavaCross-Levitate-Potion-Inv-v0\n",
            "MiniHack-LavaCross-Levitate-Ring-Pickup-v0\n",
            "MiniHack-LavaCross-Levitate-Ring-Inv-v0\n",
            "MiniHack-LavaCross-Levitate-v0\n",
            "MiniHack-LavaCross-v0\n",
            "MiniHack-Quest-Easy-v0\n",
            "MiniHack-Quest-Medium-v0\n",
            "MiniHack-Quest-Hard-v0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMRYqZCXsN4_",
        "outputId": "5045c705-266b-4a22-952c-9adf8ef83258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (5.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=53d6c1edb54795c44699379f3af03671dbd6476a1a48c8e1a5f02a06e5eb7e9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "rQqaOjJKRAqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "import nle\n",
        "import minihack\n",
        "from gym import spaces\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# I'm supressing the gym warnings but you can remove this to see them\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UrZpKoJ3RCPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if everything worked"
      ],
      "metadata": {
        "id": "biHGhy4URvEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I'm using my drive to save but I know this might not work on your end so you can replace this with your own dir, I dont know if it\n",
        "# would work if we used a shared folder.\n",
        "drive_dir_vuyo = \"drive/MyDrive/RL_Project/\" # my dir\n",
        "quest_Hard_env = gym.make(\n",
        "    \"MiniHack-Quest-Hard-v0\", \n",
        "    savedir=drive_dir_vuyo,\n",
        "    observation_keys=(\"glyphs\", \"chars\", \"colors\", \"pixel\", \"blstats\")\n",
        "    )\n",
        "reset_result = quest_Hard_env.reset()\n",
        "observation, reward, done, info = quest_Hard_env.step(1)\n",
        "print(\"info:\", info)\n",
        "quest_Hard_env.render()\n",
        "plt.imshow(observation[\"pixel\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G66M38UzRwvo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "1b191aa6-e08f-45bf-9ebe-22c72bde9fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "info: {'end_status': <StepStatus.RUNNING: 0>, 'is_ascended': False}\n",
            "\n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;37m#\u001b[0;37m<\u001b[4m\u001b[1;37m@\u001b[0m\u001b[0;37m#\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;37mA\u001b[0;37mg\u001b[0;37me\u001b[0;37mn\u001b[0;37mt\u001b[0;30m \u001b[0;37mt\u001b[0;37mh\u001b[0;37me\u001b[0;30m \u001b[0;37mT\u001b[0;37mr\u001b[0;37mo\u001b[0;37mg\u001b[0;37ml\u001b[0;37mo\u001b[0;37md\u001b[0;37my\u001b[0;37mt\u001b[0;37me\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;37mS\u001b[0;37mt\u001b[0;37m:\u001b[0;37m1\u001b[0;37m8\u001b[0;37m/\u001b[0;37m0\u001b[0;37m6\u001b[0;30m \u001b[0;37mD\u001b[0;37mx\u001b[0;37m:\u001b[0;37m1\u001b[0;37m2\u001b[0;30m \u001b[0;37mC\u001b[0;37mo\u001b[0;37m:\u001b[0;37m1\u001b[0;37m5\u001b[0;30m \u001b[0;37mI\u001b[0;37mn\u001b[0;37m:\u001b[0;37m9\u001b[0;30m \u001b[0;37mW\u001b[0;37mi\u001b[0;37m:\u001b[0;37m8\u001b[0;30m \u001b[0;37mC\u001b[0;37mh\u001b[0;37m:\u001b[0;37m7\u001b[0;30m \u001b[0;37mN\u001b[0;37me\u001b[0;37mu\u001b[0;37mt\u001b[0;37mr\u001b[0;37ma\u001b[0;37ml\u001b[0;30m \u001b[0;37mS\u001b[0;37m:\u001b[0;37m0\u001b[0;30m \u001b[0;30m \n",
            "\u001b[0;37mD\u001b[0;37ml\u001b[0;37mv\u001b[0;37ml\u001b[0;37m:\u001b[0;37m1\u001b[0;30m \u001b[0;37m$\u001b[0;37m:\u001b[0;37m0\u001b[0;30m \u001b[0;37mH\u001b[0;37mP\u001b[0;37m:\u001b[0;37m1\u001b[0;37m6\u001b[0;37m(\u001b[0;37m1\u001b[0;37m6\u001b[0;37m)\u001b[0;30m \u001b[0;37mP\u001b[0;37mw\u001b[0;37m:\u001b[0;37m2\u001b[0;37m(\u001b[0;37m2\u001b[0;37m)\u001b[0;30m \u001b[0;37mA\u001b[0;37mC\u001b[0;37m:\u001b[0;37m8\u001b[0;30m \u001b[0;37mX\u001b[0;37mp\u001b[0;37m:\u001b[0;37m1\u001b[0;37m/\u001b[0;37m0\u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0;30m \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN"
      ],
      "metadata": {
        "id": "381KHLKL_L3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Buffer"
      ],
      "metadata": {
        "id": "_SYdvl8NDnLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Initialise a buffer of a given size for storing transitions\n",
        "        :param size: the maximum number of transitions that can be stored\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            data = self._storage[i]\n",
        "            state, action, reward, next_state, done = data\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones),\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
        "        return self._encode_sample(indices)"
      ],
      "metadata": {
        "id": "hU_ZRcH7DmMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "rMdCUqQnDeIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic implementation of a Deep Q-Network. The architecture is the same as that described in the\n",
        "    Nature DQN paper.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, observation_space: spaces.Box, action_space: spaces.Discrete):\n",
        "        \"\"\"\n",
        "        Initialise the DQN\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param action_space: the action space of the environment\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert (\n",
        "            type(observation_space) == spaces.Box\n",
        "        ), \"observation_space must be of type Box\"\n",
        "        assert (\n",
        "            len(observation_space.shape) == 3\n",
        "        ), \"observation space must have the form channels x width x height\"\n",
        "        assert (\n",
        "            type(action_space) == spaces.Discrete\n",
        "        ), \"action_space must be of type Discrete\"\n",
        "        \n",
        "        \n",
        "       \n",
        "        self.conv1 = nn.Conv2d(observation_space.shape[0],32, 8,stride=4)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4,stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3,stride=1)\n",
        "    \n",
        "        self.fc1 = nn.Linear(in_features=64*7*7, out_features=512)\n",
        "        self.fc2 = nn.Linear(in_features=512, out_features=action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mYXTMFDBDdgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "EVcpA9BqDT5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        use_double_dqn,\n",
        "        lr,\n",
        "        batch_size,\n",
        "        gamma,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the DQN algorithm using the Adam optimiser\n",
        "        :param action_space: the action space of the environment\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param replay_buffer: storage for experience replay\n",
        "        :param lr: the learning rate for Adam\n",
        "        :param batch_size: the batch size\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Initialise agent's networks, optimiser and replay buffer -> Done \n",
        "        self.lr = lr\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.gamma = gamma\n",
        "        self.batch_size = batch_size\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "        #agents networks\n",
        "        self.target_network = DQN(observation_space,action_space).to(device)\n",
        "        self.policy_network = DQN(observation_space,action_space).to(device)\n",
        "        #agents optimiser \n",
        "        self.optimiser = Adam(self.policy_network.parameters(),lr=self.lr)\n",
        "        \n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        #   Optimise the TD-error over a single minibatch of transitions\n",
        "        #   Sample the minibatch from the replay-memory\n",
        "        #   using done (as a float) instead of if statement\n",
        "        #   return loss\n",
        "\n",
        "        # get batch from replay buffer and convert info into tensors\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "        tensor_states = torch.from_numpy(states/255.0).float().to(device)\n",
        "        tensor_next_states = torch.from_numpy(next_states/255.0).float().to(device)\n",
        "        tensor_actions = torch.from_numpy(actions).long().to(device)\n",
        "        tensor_rewards = torch.from_numpy(rewards).float().to(device)\n",
        "        tensor_dones = torch.from_numpy(dones).float().to(device)\n",
        "\n",
        "        # don't track gradients\n",
        "        with torch.no_grad():\n",
        "            # calculate target\n",
        "            estimate = None\n",
        "            if self.use_double_dqn:\n",
        "                # get next action from the other network\n",
        "                _, next_action = self.policy_network(tensor_states).max(1)\n",
        "                # get next q from target network\n",
        "                estimate = self.target_network(tensor_next_states).gather(1, next_action.unsqueeze(1)).squeeze()\n",
        "            else:\n",
        "                estimate = None\n",
        "              \n",
        "                # get next q from target network\n",
        "                \n",
        "\n",
        "        estimate = self.target_network(tensor_next_states).max(1)\n",
        "\n",
        "        c = self.gamma * estimate[0]\n",
        "        target = tensor_rewards + (1 - tensor_dones) * c\n",
        "\n",
        "        # backpropagation\n",
        "        new_estimate = self.policy_network(tensor_states).gather(1, tensor_actions.unsqueeze(1)).squeeze()\n",
        "        loss = nn.functional.l1_loss(new_estimate, target)\n",
        "        self.optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimiser.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        # TODO update target_network parameters with policy_network parameters -> Done \n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "    def act(self, state: np.ndarray):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        # make rgb values have a range of [0,1]\n",
        "        state = np.array(state)/255.0\n",
        "        # convert state to tensor object and put on GPU\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        # making sure gradients aren't saved for the following calculations\n",
        "        with torch.no_grad():\n",
        "            # get action-state values using the foward pass of the network\n",
        "            qs = self.policy_network(state)\n",
        "            # get max action\n",
        "            _, action = qs.max(1)\n",
        "            # return action from tensor object\n",
        "            return action.item()"
      ],
      "metadata": {
        "id": "nZdmqtVhDTFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "xiXp80KyDQUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    hyper_params = {\n",
        "        \"seed\": 42,  # which seed to use\n",
        "        \"env\": \"MiniHack-Quest-Hard-v0\",  # name of the game\n",
        "        \"replay-buffer-size\": int(5e3),  # replay buffer size\n",
        "        \"learning-rate\": 1e-4,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(1e6),  # total number of steps to run the environment for\n",
        "        \"batch-size\": 32,  # number of transitions to optimize at the same time\n",
        "        \"learning-starts\": 10000,  # number of steps before learning starts\n",
        "        \"learning-freq\": 1,  # number of iterations between every optimization step\n",
        "        \"use-double-dqn\": False,  # use double deep Q-learning\n",
        "        \"target-update-freq\": 1000,  # number of iterations between every target network update\n",
        "        \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "        \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "        \"eps-fraction\": 0.1,  # fraction of num-steps\n",
        "        \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    assert \"NoFrameskip\" in hyper_params[\"env\"], \"Require environment with no frameskip\"\n",
        "    \n",
        "    env = gym.make(hyper_params[\"env\"])\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    # TODO Pick Gym wrappers to use\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = WarpFrame(env) #-> warp frame to 84x84\n",
        "    env = PyTorchFrame(env) # turn to channel x height xwidth dimension for pytorch \n",
        "    env = FrameStack(env,4) #take the last 4 frames \n",
        "    # env = ClipRewardEnv(env)\n",
        "    # record video every 50 episodes\n",
        "    env = gym.wrappers.Monitor(env, '/content/drive/Shareddrives/Reinforcement_Learning/project/', video_callable=lambda episode_id: episode_id % 50 == 0, force=True)\n",
        "    \n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    # TODO Create dqn agent\n",
        "    agent = DQNAgent(env.observation_space,\n",
        "        env.action_space,\n",
        "        replay_buffer,\n",
        "        use_double_dqn=hyper_params[\"use-double-dqn\"],\n",
        "        lr=hyper_params[\"learning-rate\"],\n",
        "        batch_size= hyper_params[\"batch-size\"],\n",
        "        gamma=hyper_params[\"discount-factor\"])\n",
        "\n",
        "    eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
        "    episode_rewards = [0.0]\n",
        "\n",
        "    state = env.reset()\n",
        "    for t in range(hyper_params[\"num-steps\"]):\n",
        "        fraction = min(1.0, float(t) / eps_timesteps)\n",
        "        eps_threshold = hyper_params[\"eps-start\"] + fraction * (\n",
        "            hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"]\n",
        "        )\n",
        "        sample = random.random()\n",
        "        #  select random action if sample is less equal than eps_threshold\n",
        "        # take step in env\n",
        "        # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "        # add reward to episode_reward\n",
        "        # action = None\n",
        "        if sample < eps_threshold:\n",
        "            action = random.randrange(env.action_space.n)\n",
        "        else:\n",
        "            action = agent.act(state)\n",
        "\n",
        "        #take next step\n",
        "        next_state, reward, done, _ = env.step(action) \n",
        "        agent.replay_buffer.add(state, action, reward, next_state, float(done))\n",
        "        state = next_state\n",
        "\n",
        "        episode_rewards[-1] += reward\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"learning-freq\"] == 0\n",
        "        ):\n",
        "            agent.optimise_td_loss()\n",
        "\n",
        "        if (\n",
        "            t > hyper_params[\"learning-starts\"]\n",
        "            and t % hyper_params[\"target-update-freq\"] == 0\n",
        "        ):\n",
        "            agent.update_target_network()\n",
        "\n",
        "        num_episodes = len(episode_rewards)\n",
        "\n",
        "        if (\n",
        "            done\n",
        "            and hyper_params[\"print-freq\"] is not None\n",
        "            and len(episode_rewards) % hyper_params[\"print-freq\"] == 0\n",
        "        ):\n",
        "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "            print(\"********************************************************\")\n",
        "            print(\"steps: {}\".format(t))\n",
        "            print(\"episodes: {}\".format(num_episodes))\n",
        "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "            print(\"********************************************************\")\n",
        "            np.savetxt('/content/drive/Shareddrives/Reinforcement_Learning/rewards.csv', episode_rewards)"
      ],
      "metadata": {
        "id": "2p8_Co9g_NVx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}