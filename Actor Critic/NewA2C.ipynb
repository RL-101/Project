{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOHwSmNLOT_r",
        "outputId": "50281487-ab4a-4fb0-af1c-20b33e4f0003"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [C\u001b[0m\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [2 InRelease 14.2 kB/88.\u001b[0m\u001b[33m\r0% [Waiting for headers] [2 InRelease 48.9 kB/88.7 kB 55%] [Connected to cloud.\u001b[0m\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [2 InRelease 48.9 kB/88.7 kB 55%] [Connected to cloud.\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [2 InRelease 53.3 kB/88.7 k\u001b[0m\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [2 InRelease 76.4 kB/88.7 k\u001b[0m\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [2 InRelease 76.4 kB/88.7 k\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [2 InRelease 76.4 kB/88.7 k\u001b[0m\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [6 InRelease 14.2 kB/88.7 kB 16%] [2 InRelease 76\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [6 InRelease 15.6 kB/88.7 kB 18%] [Waiting for he\u001b[0m\r                                                                               \rGet:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [6 InRelease 53.3 kB/88.7 kB 60%] [Waiting for he\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [7 InRelease 14.2 kB/15.9 k\u001b[0m\u001b[33m\r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Waiting for headers] [Conn\u001b[0m\r                                                                               \rGet:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,184 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,118 kB]\n",
            "Get:15 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [98.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,035 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,467 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [992 kB]\n",
            "Fetched 15.1 MB in 2s (7,438 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  automake autotools-dev file libmagic-mgc libmagic1 libsigsegv2 m4\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc gettext libtool-doc gcj-jdk\n",
            "  m4-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autotools-dev file libmagic-mgc libmagic1 libsigsegv2\n",
            "  libtool m4\n",
            "0 upgraded, 9 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,551 kB of archives.\n",
            "After this operation, 10.5 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Fetched 1,551 kB in 0s (13.3 MB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../1-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../2-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "Preparing to unpack .../3-libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../4-m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../5-autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../6-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../7-automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../8-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n",
            "libbz2-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libbison-dev libfl-dev libfl2\n",
            "Suggested packages:\n",
            "  bison-doc flex-doc\n",
            "The following NEW packages will be installed:\n",
            "  bison flex libbison-dev libfl-dev libfl2\n",
            "0 upgraded, 5 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 938 kB of archives.\n",
            "After this operation, 2,925 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 flex amd64 2.6.4-6 [316 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libbison-dev amd64 2:3.0.4.dfsg-1build1 [339 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 bison amd64 2:3.0.4.dfsg-1build1 [266 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfl2 amd64 2.6.4-6 [11.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfl-dev amd64 2.6.4-6 [6,320 B]\n",
            "Fetched 938 kB in 0s (8,503 kB/s)\n",
            "Selecting previously unselected package flex.\n",
            "(Reading database ... 124284 files and directories currently installed.)\n",
            "Preparing to unpack .../flex_2.6.4-6_amd64.deb ...\n",
            "Unpacking flex (2.6.4-6) ...\n",
            "Selecting previously unselected package libbison-dev:amd64.\n",
            "Preparing to unpack .../libbison-dev_2%3a3.0.4.dfsg-1build1_amd64.deb ...\n",
            "Unpacking libbison-dev:amd64 (2:3.0.4.dfsg-1build1) ...\n",
            "Selecting previously unselected package bison.\n",
            "Preparing to unpack .../bison_2%3a3.0.4.dfsg-1build1_amd64.deb ...\n",
            "Unpacking bison (2:3.0.4.dfsg-1build1) ...\n",
            "Selecting previously unselected package libfl2:amd64.\n",
            "Preparing to unpack .../libfl2_2.6.4-6_amd64.deb ...\n",
            "Unpacking libfl2:amd64 (2.6.4-6) ...\n",
            "Selecting previously unselected package libfl-dev:amd64.\n",
            "Preparing to unpack .../libfl-dev_2.6.4-6_amd64.deb ...\n",
            "Unpacking libfl-dev:amd64 (2.6.4-6) ...\n",
            "Setting up flex (2.6.4-6) ...\n",
            "Setting up libbison-dev:amd64 (2:3.0.4.dfsg-1build1) ...\n",
            "Setting up libfl2:amd64 (2.6.4-6) ...\n",
            "Setting up bison (2:3.0.4.dfsg-1build1) ...\n",
            "update-alternatives: using /usr/bin/bison.yacc to provide /usr/bin/yacc (yacc) in auto mode\n",
            "Setting up libfl-dev:amd64 (2.6.4-6) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nle\n",
            "  Downloading nle-0.8.1.tar.gz (6.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.9 MB 13.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.15 in /usr/local/lib/python3.7/dist-packages (from nle) (0.25.2)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from nle) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.15->nle) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym>=0.15->nle) (4.1.1)\n",
            "Building wheels for collected packages: nle\n",
            "  Building wheel for nle (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nle: filename=nle-0.8.1-cp37-cp37m-linux_x86_64.whl size=2883127 sha256=b30879d382a36f02c074f24605fc4c69dab14d1311ef55414317e5aa258c8383\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/43/b7/00eec64b2f64dc45883624bcb42a969645c86814ea751c6299\n",
            "Successfully built nle\n",
            "Installing collected packages: pybind11, nle\n",
            "Successfully installed nle-0.8.1 pybind11-2.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting minihack\n",
            "  Downloading minihack-0.1.3.tar.gz (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 14.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nle>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from minihack) (0.8.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from minihack) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from minihack) (1.21.6)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.7/dist-packages (from nle>=0.8.0->minihack) (2.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->minihack) (1.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->minihack) (4.1.1)\n",
            "Building wheels for collected packages: minihack\n",
            "  Building wheel for minihack (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for minihack: filename=minihack-0.1.3-py3-none-any.whl size=261792 sha256=c98a9938c94f0bf8edb9ed954b954f79305480045d1b64acca17756678cb93a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/b0/50/bb8c09fe5befa92b343025c26d614c5fa312f1edb432cc9580\n",
            "Successfully built minihack\n",
            "Installing collected packages: minihack\n",
            "Successfully installed minihack-0.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.13.0)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=a1cb2adade0ed7734ff0640b4996a1db1c9077fbc50889bbc0ec8bd772b51cf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!apt install -y cmake\n",
        "!apt-get install -y build-essential autoconf libtool pkg-config\n",
        "!apt-get install flex bison libbz2-dev\n",
        "!pip install nle\n",
        "!pip install minihack\n",
        "# !python -m minihack.scripts.env_list\n",
        "!pip install gym[atari,accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import gym\n",
        "import minihack \n",
        "from nle import nethack \n",
        "\n",
        "import numpy as np \n",
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from torch import flatten\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.nn import Sequential, Conv2d, MaxPool2d, Module\n",
        "from torch.nn import Softmax, Dropout, Sequential"
      ],
      "metadata": {
        "id": "fXAfSGJTOZBA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_state(state):\n",
        "    \"\"\"Formats the state according to the input requirements of the Actor Critic Neural Network\"\"\"\n",
        "    \n",
        "    # Normalize and reshape for convolutional layer input\n",
        "    glyphs = state[\"glyphs\"]\n",
        "    glyphs = glyphs/glyphs.max()\n",
        "    glyphs = glyphs.reshape((1,1,21,79))\n",
        "    \n",
        "    # Normalize the message and reshape for the fully connected layer input \n",
        "    message = state[\"message\"]\n",
        "    if state[\"message\"].max()>0: \n",
        "        # Occassionally the message is empty which will cause a Zero Division error \n",
        "        message = message/message.max()\n",
        "    message = message.reshape((1,len(message)))\n",
        "    \n",
        "    state = {\"glyphs\":glyphs,\"message\":message}\n",
        "    return state"
      ],
      "metadata": {
        "id": "EPB-c2hnTAJ-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cooked\n",
        "def compute_returns(rewards, gamma=0.99):\n",
        "    \"calculating discounted rewards\"\n",
        "    returns = []\n",
        "    dis_reward= 0\n",
        "    for reward in rewards[::-1]:\n",
        "        dis_reward = reward + gamma*dis_reward\n",
        "        returns.insert(0,dis_reward) #reversed \n",
        "    \n",
        "    #normalizing the rewards:\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std())\n",
        "\n",
        "    return returns"
      ],
      "metadata": {
        "id": "E5qIHMoaTBs2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(name,rewards):\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    ave = np.mean(rewards,axis=0)\n",
        "    plt.plot(ave,label=\"Average Reward\")\n",
        "    \n",
        "    plt.title(f\"{name}\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VHf-UNNvTFgp"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "AQLRfE8jTUfA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.reshape(input.shape[0], -1)"
      ],
      "metadata": {
        "id": "118GxCQg4M9X"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic3(nn.Module):\n",
        "    \"\"\"The Actor Critic Neural Network used to estimate the state value function and action probabilities\"\"\"\n",
        "    def __init__(self,s_size=8,h_size=128, a_size=4):\n",
        "\n",
        "        # The network architecture follows the popular lenet-5 CNN architeture \n",
        "        super(ActorCritic3, self).__init__()\n",
        "\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.state_values  = []\n",
        "\n",
        "        # Initialize first convolutional layer\n",
        "        self.conv1 = Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5))\n",
        "\n",
        "       \n",
        "        # remaining convolutions, relus, pools, and 3 FC networks\n",
        "        self.conv2_fc = Sequential(\n",
        "                            ReLU(),\n",
        "                            MaxPool2d(kernel_size=(3, 3), stride=(2, 2)),\n",
        "\n",
        "                            # conv2\n",
        "                            Conv2d(in_channels=16, out_channels=52,kernel_size=(5, 5)),\n",
        "                            ReLU(),\n",
        "                            MaxPool2d(kernel_size=(3, 3), stride=(2, 2)),\n",
        "\n",
        "                            # Flatten the output from the final pooling layer and pass it through the fully connected layers\n",
        "                            Flatten(),\n",
        "\n",
        "                            # fully connected layers for glyph output after convolutional and pooling layers\n",
        "                            # fc1\n",
        "                            Linear(in_features=832, out_features=500),\n",
        "                            ReLU(),\n",
        "\n",
        "                            # fc2\n",
        "                            Linear(in_features=500, out_features=128),\n",
        "                            ReLU(),\n",
        "\n",
        "                          ) \n",
        "\n",
        "        # fully connected network for message input\n",
        "        # fc3\n",
        "        self.message_fc = Sequential(\n",
        "                            Linear(in_features=256, out_features=128),\n",
        "                            ReLU(),\n",
        "                          )\n",
        "        \n",
        "        #  fully connected for combination of glyphs and message \n",
        "        self.combined_fc = Sequential(\n",
        "                            Linear(in_features=256, out_features=128),\n",
        "                            ReLU()\n",
        "                          )\n",
        "\n",
        "        # To estimate the value function of the state \n",
        "        self.value_layer = Linear(128, 1)\n",
        "\n",
        "        # To calculate the probability of taking each action in the given state\n",
        "        self.action_layer = Sequential(\n",
        "                              Linear(128, a_size),\n",
        "                              Softmax(dim=-1)\n",
        "                            )\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        \n",
        "        # Transform the glyph and state arrays into tensors \n",
        "        glyphs_t  = torch.from_numpy(state[\"glyphs\"]).float().to(device)\n",
        "        message_t  = torch.from_numpy(state[\"message\"]).float().to(device)\n",
        "\n",
        "        # Pass the 2D glyphs input through our convolutional and pooling layers\n",
        "        glyphs_t = self.conv2_fc(self.conv1(glyphs_t))\n",
        "\n",
        "        # Pass the message input through a fully connected layer\n",
        "        message_t = self.message_fc(message_t)\n",
        "\n",
        "        # Combine glyphs output from convolution and fully connected layers \n",
        "        # with message output from fully connected layer \n",
        "        # Cat and Concat are used for different versions of PyTorch\n",
        "        try:\n",
        "            combined = torch.cat((glyphs_t,message_t),1)\n",
        "        except:\n",
        "            combined = torch.concat([glyphs_t,message_t],1)\n",
        "        \n",
        "        # Pass glyphs and messaged combination through a fully connected layer\n",
        "        combined = self.combined_fc(combined)\n",
        "\n",
        "        state_value = self.value_layer(combined)\n",
        "        \n",
        "        action_probs = self.action_layer(combined)\n",
        "\n",
        "        # a list with the probability of each action over the action space\n",
        "        action_distribution = Categorical(action_probs)\n",
        "        # select action\n",
        "        action = action_distribution.sample()\n",
        "\n",
        "        self.log_probs.append(action_distribution.log_prob(action))\n",
        "        self.state_values.append(state_value)\n",
        "        return action.item()\n",
        "      \n",
        "    \n",
        "    def calculateLoss(self, gamma, returns):\n",
        "    \n",
        "        loss = 0\n",
        "        for logprob, value, reward in zip(self.log_probs, self.state_values, returns):\n",
        "            advantage = reward  - value.item()\n",
        "            action_loss = -logprob * advantage\n",
        "            value_loss = F.smooth_l1_loss(value, reward)\n",
        "            loss += (action_loss + value_loss)   \n",
        "        return loss\n",
        "\n",
        "    def reset(self):\n",
        "        del self.log_probs[:]\n",
        "        del self.state_values[:]\n",
        "        del self.rewards[:]\n"
      ],
      "metadata": {
        "id": "NM61QM7MZlzw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# state = torch.from_numpy(state[\"glyphs\"]).float().to(device)\n",
        "      \n",
        "# flatshape = self.flat_shape(state.shape)\n",
        "def flat_shape(self,input): #if you want to determine flatshape \n",
        "\n",
        "      x = torch.zeros(*input)\n",
        "      x = self.conv1(x)\n",
        "      x = self.maxpool1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.maxpool1(x)\n",
        "\n",
        "      return int(np.prod(x.size()))"
      ],
      "metadata": {
        "id": "YVM1bwi05Urs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSeeds(it):\n",
        "  return np.random.randint(1000, size=it) #5 interations "
      ],
      "metadata": {
        "id": "7-Q7qPYeFQr2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_path = '/content/drive/MyDrive/RL_Project/A2C.pth'\n",
        "def save_model_weights(network):\n",
        "    \"saves the actor and critic network weights\"\n",
        "    torch.save(network.state_dict(),network_path)"
      ],
      "metadata": {
        "id": "9kDsnYcxkWqj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(env, model, seed):\n",
        "\n",
        "    learning_rate = 0.02\n",
        "    num_episodes = 100\n",
        "    max_episodes = 1000\n",
        "    gamma = 0.99\n",
        "  \n",
        "    # Setting random seeds (for reproducibility)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    env.seed(seed)\n",
        "    \n",
        "    # Initialize optimizer for Actor Critic Network\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    # List to store the rewards attained in each episode \n",
        "    model_rewards =[]\n",
        "    \n",
        "    for i in range(num_episodes):\n",
        "        # Reset environment\n",
        "        state = format_state(env.reset())\n",
        "        # Flag to see if episode has terminated\n",
        "        done = False\n",
        " \n",
        "        \n",
        "        for t in range(1, max_episodes):\n",
        "            #COOKED\n",
        "            action = model.forward(state)\n",
        "            \n",
        "            # Take selected action, observe the reward received, the next state \n",
        "            # and whether or not the episode terminated \n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = format_state(next_state)\n",
        "            \n",
        "            # COOKED\n",
        "            # Store the reward\n",
        "            model.rewards.append(reward)\n",
        "            \n",
        "            state = next_state\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Store the reward acquired in the episode and calculate the discounted return of the episode\n",
        "        model_rewards.append(np.sum(model.rewards))\n",
        "        returns = compute_returns(model.rewards, gamma)\n",
        "        returns = torch.from_numpy(returns).float().to(device)\n",
        "        \n",
        "        # Print the episode, the reward acquired in the episode and the mean reward over the last 50 episodes \n",
        "        \n",
        "        print(\"Episode:\",i,\"Reward:\",np.sum(model.rewards),\"Average Reward:\",np.mean(model_rewards[-50:]),\"Steps\",t)\n",
        "        \n",
        "        # COOKED \n",
        "        loss = model.calculateLoss(gamma, returns)   \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        model.reset()\n",
        "\n",
        "    # Store the policy as the neural network model at the final iteration \n",
        "    policy = model\n",
        "\n",
        "    return policy, model_rewards"
      ],
      "metadata": {
        "id": "_Gt2RPevTKdH"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on MiniHack-Room-5x5-v0"
      ],
      "metadata": {
        "id": "_1jf8K30nn5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"MiniHack-Room-5x5-v0\", observation_keys=[\"glyphs\",\"pixel\",\"message\"],max_episode_steps=1000)\n",
        "room_rewards = []\n",
        "state = format_state(env.reset())\n",
        "seeds = getSeeds(5) #average over 5 runs\n",
        "for seed in seeds:\n",
        "  act_critic_model = ActorCritic3(h_size=512, a_size=env.action_space.n)\n",
        "  policy, model_rewards = run(env=env,model = act_critic_model, seed = seed )\n",
        "  room_rewards.append(model_rewards)\n",
        "\n",
        "plot_results(\"MiniHack-Room-5x5-v0\",room_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cl0i_Cc3Tj0H",
        "outputId": "df5e43c6-b61c-4b85-971b-7226c2126b69"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Reward: -0.25 Average Reward: -0.25 Steps 100\n",
            "Episode: 1 Reward: 0.8099999999999999 Average Reward: 0.27999999999999997 Steps 39\n",
            "Episode: 2 Reward: 0.7 Average Reward: 0.41999999999999993 Steps 59\n",
            "Episode: 3 Reward: -0.96 Average Reward: 0.07499999999999996 Steps 100\n",
            "Episode: 4 Reward: 0.87 Average Reward: 0.23399999999999999 Steps 21\n",
            "Episode: 5 Reward: 0.97 Average Reward: 0.35666666666666663 Steps 15\n",
            "Episode: 6 Reward: 0.9299999999999999 Average Reward: 0.4385714285714285 Steps 16\n",
            "Episode: 7 Reward: 1.0 Average Reward: 0.5087499999999999 Steps 4\n",
            "Episode: 8 Reward: 1.0 Average Reward: 0.5633333333333332 Steps 6\n",
            "Episode: 9 Reward: 1.0 Average Reward: 0.607 Steps 4\n",
            "Episode: 10 Reward: 0.92 Average Reward: 0.6354545454545454 Steps 20\n",
            "Episode: 11 Reward: 0.75 Average Reward: 0.6449999999999999 Steps 33\n",
            "Episode: 12 Reward: 1.0 Average Reward: 0.6723076923076922 Steps 4\n",
            "Episode: 13 Reward: 1.0 Average Reward: 0.6957142857142856 Steps 4\n",
            "Episode: 14 Reward: -0.96 Average Reward: 0.5853333333333332 Steps 100\n",
            "Episode: 15 Reward: 1.0 Average Reward: 0.61125 Steps 4\n",
            "Episode: 16 Reward: 1.0 Average Reward: 0.6341176470588235 Steps 4\n",
            "Episode: 17 Reward: 1.0 Average Reward: 0.6544444444444444 Steps 4\n",
            "Episode: 18 Reward: 1.0 Average Reward: 0.6726315789473684 Steps 4\n",
            "Episode: 19 Reward: 1.0 Average Reward: 0.689 Steps 4\n",
            "Episode: 20 Reward: 1.0 Average Reward: 0.7038095238095238 Steps 4\n",
            "Episode: 21 Reward: -0.96 Average Reward: 0.6281818181818182 Steps 100\n",
            "Episode: 22 Reward: 1.0 Average Reward: 0.6443478260869565 Steps 4\n",
            "Episode: 23 Reward: -0.96 Average Reward: 0.5775 Steps 100\n",
            "Episode: 24 Reward: -0.94 Average Reward: 0.5168 Steps 100\n",
            "Episode: 25 Reward: 0.3999999999999999 Average Reward: 0.5123076923076924 Steps 66\n",
            "Episode: 26 Reward: -0.94 Average Reward: 0.45851851851851855 Steps 100\n",
            "Episode: 27 Reward: -0.8699999999999999 Average Reward: 0.41107142857142864 Steps 100\n",
            "Episode: 28 Reward: -0.9099999999999999 Average Reward: 0.3655172413793104 Steps 100\n",
            "Episode: 29 Reward: 1.0 Average Reward: 0.3866666666666667 Steps 10\n",
            "Episode: 30 Reward: -0.8499999999999999 Average Reward: 0.3467741935483872 Steps 100\n",
            "Episode: 31 Reward: -0.9199999999999999 Average Reward: 0.3071875 Steps 100\n",
            "Episode: 32 Reward: 0.31999999999999984 Average Reward: 0.3075757575757576 Steps 79\n",
            "Episode: 33 Reward: -0.7 Average Reward: 0.27794117647058825 Steps 100\n",
            "Episode: 34 Reward: 0.43999999999999995 Average Reward: 0.2825714285714286 Steps 73\n",
            "Episode: 35 Reward: -0.85 Average Reward: 0.2511111111111111 Steps 100\n",
            "Episode: 36 Reward: 0.3899999999999999 Average Reward: 0.2548648648648649 Steps 75\n",
            "Episode: 37 Reward: -0.8999999999999999 Average Reward: 0.22447368421052635 Steps 100\n",
            "Episode: 38 Reward: -0.86 Average Reward: 0.19666666666666668 Steps 100\n",
            "Episode: 39 Reward: 0.9299999999999999 Average Reward: 0.215 Steps 19\n",
            "Episode: 40 Reward: 1.0 Average Reward: 0.23414634146341462 Steps 4\n",
            "Episode: 41 Reward: 1.0 Average Reward: 0.2523809523809524 Steps 4\n",
            "Episode: 42 Reward: 0.9 Average Reward: 0.26744186046511625 Steps 26\n",
            "Episode: 43 Reward: 0.94 Average Reward: 0.2827272727272727 Steps 18\n",
            "Episode: 44 Reward: 0.74 Average Reward: 0.29288888888888887 Steps 46\n",
            "Episode: 45 Reward: 0.9 Average Reward: 0.3060869565217391 Steps 37\n",
            "Episode: 46 Reward: 0.98 Average Reward: 0.32042553191489365 Steps 12\n",
            "Episode: 47 Reward: 0.77 Average Reward: 0.32979166666666665 Steps 37\n",
            "Episode: 48 Reward: 0.97 Average Reward: 0.34285714285714286 Steps 23\n",
            "Episode: 49 Reward: 0.99 Average Reward: 0.3558 Steps 9\n",
            "Episode: 50 Reward: 0.86 Average Reward: 0.3779999999999999 Steps 50\n",
            "Episode: 51 Reward: 0.98 Average Reward: 0.38139999999999996 Steps 11\n",
            "Episode: 52 Reward: 1.0 Average Reward: 0.3874 Steps 4\n",
            "Episode: 53 Reward: 1.0 Average Reward: 0.4266 Steps 6\n",
            "Episode: 54 Reward: 1.0 Average Reward: 0.4292 Steps 4\n",
            "Episode: 55 Reward: 1.0 Average Reward: 0.4298 Steps 4\n",
            "Episode: 56 Reward: 0.99 Average Reward: 0.43099999999999994 Steps 7\n",
            "Episode: 57 Reward: 0.97 Average Reward: 0.4303999999999999 Steps 10\n",
            "Episode: 58 Reward: -0.62 Average Reward: 0.3979999999999999 Steps 100\n",
            "Episode: 59 Reward: 1.0 Average Reward: 0.39799999999999996 Steps 4\n",
            "Episode: 60 Reward: 1.0 Average Reward: 0.3996 Steps 4\n",
            "Episode: 61 Reward: 1.0 Average Reward: 0.4046 Steps 4\n",
            "Episode: 62 Reward: 0.7499999999999999 Average Reward: 0.39959999999999996 Steps 40\n",
            "Episode: 63 Reward: 0.95 Average Reward: 0.3985999999999999 Steps 13\n",
            "Episode: 64 Reward: 0.92 Average Reward: 0.43620000000000003 Steps 14\n",
            "Episode: 65 Reward: 0.99 Average Reward: 0.436 Steps 8\n",
            "Episode: 66 Reward: 1.0 Average Reward: 0.43599999999999994 Steps 4\n",
            "Episode: 67 Reward: 1.0 Average Reward: 0.43599999999999994 Steps 4\n",
            "Episode: 68 Reward: 0.99 Average Reward: 0.43579999999999997 Steps 11\n",
            "Episode: 69 Reward: 1.0 Average Reward: 0.43579999999999997 Steps 8\n",
            "Episode: 70 Reward: 1.0 Average Reward: 0.43579999999999997 Steps 4\n",
            "Episode: 71 Reward: 1.0 Average Reward: 0.475 Steps 4\n",
            "Episode: 72 Reward: 1.0 Average Reward: 0.475 Steps 4\n",
            "Episode: 73 Reward: 0.82 Average Reward: 0.5106 Steps 34\n",
            "Episode: 74 Reward: 0.99 Average Reward: 0.5491999999999999 Steps 9\n",
            "Episode: 75 Reward: 0.8099999999999999 Average Reward: 0.5573999999999999 Steps 36\n",
            "Episode: 76 Reward: 0.97 Average Reward: 0.5955999999999999 Steps 12\n",
            "Episode: 77 Reward: 0.97 Average Reward: 0.6324 Steps 9\n",
            "Episode: 78 Reward: 0.79 Average Reward: 0.6664 Steps 33\n",
            "Episode: 79 Reward: 1.0 Average Reward: 0.6664 Steps 4\n",
            "Episode: 80 Reward: 1.0 Average Reward: 0.7034 Steps 4\n",
            "Episode: 81 Reward: 1.0 Average Reward: 0.7417999999999999 Steps 4\n",
            "Episode: 82 Reward: 1.0 Average Reward: 0.7554 Steps 4\n",
            "Episode: 83 Reward: 1.0 Average Reward: 0.7894 Steps 4\n",
            "Episode: 84 Reward: 1.0 Average Reward: 0.8006 Steps 4\n",
            "Episode: 85 Reward: 1.0 Average Reward: 0.8375999999999999 Steps 4\n",
            "Episode: 86 Reward: 1.0 Average Reward: 0.8497999999999999 Steps 4\n",
            "Episode: 87 Reward: 0.99 Average Reward: 0.8876000000000001 Steps 6\n",
            "Episode: 88 Reward: 1.0 Average Reward: 0.9248000000000001 Steps 6\n",
            "Episode: 89 Reward: 1.0 Average Reward: 0.9262 Steps 4\n",
            "Episode: 90 Reward: 0.99 Average Reward: 0.926 Steps 7\n",
            "Episode: 91 Reward: 0.9299999999999999 Average Reward: 0.9246000000000001 Steps 15\n",
            "Episode: 92 Reward: 1.0 Average Reward: 0.9266 Steps 4\n",
            "Episode: 93 Reward: 1.0 Average Reward: 0.9278 Steps 4\n",
            "Episode: 94 Reward: 1.0 Average Reward: 0.9329999999999999 Steps 4\n",
            "Episode: 95 Reward: 1.0 Average Reward: 0.935 Steps 4\n",
            "Episode: 96 Reward: 0.88 Average Reward: 0.9329999999999999 Steps 20\n",
            "Episode: 97 Reward: 1.0 Average Reward: 0.9376000000000001 Steps 4\n",
            "Episode: 98 Reward: 1.0 Average Reward: 0.9381999999999999 Steps 4\n",
            "Episode: 99 Reward: 1.0 Average Reward: 0.9384 Steps 4\n",
            "Episode: 0 Reward: -0.41000000000000003 Average Reward: -0.41000000000000003 Steps 100\n",
            "Episode: 1 Reward: 0.91 Average Reward: 0.25 Steps 26\n",
            "Episode: 2 Reward: 0.98 Average Reward: 0.49333333333333335 Steps 9\n",
            "Episode: 3 Reward: 0.97 Average Reward: 0.6125 Steps 22\n",
            "Episode: 4 Reward: 0.95 Average Reward: 0.68 Steps 17\n",
            "Episode: 5 Reward: 0.9 Average Reward: 0.7166666666666668 Steps 23\n",
            "Episode: 6 Reward: 0.96 Average Reward: 0.7514285714285716 Steps 14\n",
            "Episode: 7 Reward: 0.96 Average Reward: 0.7775000000000001 Steps 15\n",
            "Episode: 8 Reward: 0.77 Average Reward: 0.7766666666666667 Steps 36\n",
            "Episode: 9 Reward: 1.0 Average Reward: 0.799 Steps 12\n",
            "Episode: 10 Reward: 0.9299999999999999 Average Reward: 0.8109090909090909 Steps 20\n",
            "Episode: 11 Reward: 0.91 Average Reward: 0.8191666666666667 Steps 27\n",
            "Episode: 12 Reward: 0.85 Average Reward: 0.8215384615384616 Steps 34\n",
            "Episode: 13 Reward: 0.94 Average Reward: 0.83 Steps 23\n",
            "Episode: 14 Reward: 0.98 Average Reward: 0.84 Steps 13\n",
            "Episode: 15 Reward: 0.9299999999999999 Average Reward: 0.8456250000000001 Steps 23\n",
            "Episode: 16 Reward: 1.0 Average Reward: 0.8547058823529412 Steps 13\n",
            "Episode: 17 Reward: 0.95 Average Reward: 0.86 Steps 19\n",
            "Episode: 18 Reward: 0.8099999999999999 Average Reward: 0.8573684210526316 Steps 28\n",
            "Episode: 19 Reward: 0.94 Average Reward: 0.8615 Steps 13\n",
            "Episode: 20 Reward: 0.91 Average Reward: 0.8638095238095238 Steps 17\n",
            "Episode: 21 Reward: 0.8 Average Reward: 0.860909090909091 Steps 27\n",
            "Episode: 22 Reward: 0.5399999999999999 Average Reward: 0.8469565217391305 Steps 54\n",
            "Episode: 23 Reward: 0.7799999999999999 Average Reward: 0.8441666666666666 Steps 30\n",
            "Episode: 24 Reward: 0.86 Average Reward: 0.8447999999999999 Steps 22\n",
            "Episode: 25 Reward: 0.7799999999999999 Average Reward: 0.8423076923076922 Steps 30\n",
            "Episode: 26 Reward: 0.99 Average Reward: 0.8477777777777776 Steps 9\n",
            "Episode: 27 Reward: 0.96 Average Reward: 0.8517857142857143 Steps 12\n",
            "Episode: 28 Reward: 0.99 Average Reward: 0.8565517241379309 Steps 9\n",
            "Episode: 29 Reward: 0.74 Average Reward: 0.8526666666666665 Steps 34\n",
            "Episode: 30 Reward: 1.0 Average Reward: 0.8574193548387095 Steps 8\n",
            "Episode: 31 Reward: 0.7599999999999999 Average Reward: 0.854375 Steps 32\n",
            "Episode: 32 Reward: 0.7 Average Reward: 0.8496969696969696 Steps 41\n",
            "Episode: 33 Reward: 1.0 Average Reward: 0.8541176470588235 Steps 7\n",
            "Episode: 34 Reward: 0.96 Average Reward: 0.8571428571428571 Steps 13\n",
            "Episode: 35 Reward: 0.98 Average Reward: 0.8605555555555555 Steps 12\n",
            "Episode: 36 Reward: 0.91 Average Reward: 0.8618918918918919 Steps 20\n",
            "Episode: 37 Reward: 0.98 Average Reward: 0.8649999999999999 Steps 14\n",
            "Episode: 38 Reward: 0.99 Average Reward: 0.8682051282051282 Steps 9\n",
            "Episode: 39 Reward: 0.97 Average Reward: 0.8707499999999999 Steps 10\n",
            "Episode: 40 Reward: 0.99 Average Reward: 0.8736585365853659 Steps 13\n",
            "Episode: 41 Reward: 0.9299999999999999 Average Reward: 0.875 Steps 22\n",
            "Episode: 42 Reward: 0.9099999999999999 Average Reward: 0.875813953488372 Steps 24\n",
            "Episode: 43 Reward: 0.99 Average Reward: 0.8784090909090909 Steps 7\n",
            "Episode: 44 Reward: 0.92 Average Reward: 0.8793333333333333 Steps 29\n",
            "Episode: 45 Reward: 0.97 Average Reward: 0.8813043478260869 Steps 13\n",
            "Episode: 46 Reward: 0.97 Average Reward: 0.8831914893617021 Steps 11\n",
            "Episode: 47 Reward: 0.8200000000000001 Average Reward: 0.881875 Steps 41\n",
            "Episode: 48 Reward: 0.94 Average Reward: 0.8830612244897958 Steps 17\n",
            "Episode: 49 Reward: 0.81 Average Reward: 0.8815999999999999 Steps 34\n",
            "Episode: 50 Reward: 0.99 Average Reward: 0.9096000000000001 Steps 9\n",
            "Episode: 51 Reward: 0.91 Average Reward: 0.9096 Steps 23\n",
            "Episode: 52 Reward: 0.9 Average Reward: 0.9079999999999999 Steps 20\n",
            "Episode: 53 Reward: 0.91 Average Reward: 0.9067999999999999 Steps 20\n",
            "Episode: 54 Reward: 0.91 Average Reward: 0.9059999999999999 Steps 23\n",
            "Episode: 55 Reward: 0.98 Average Reward: 0.9076 Steps 13\n",
            "Episode: 56 Reward: 0.6499999999999999 Average Reward: 0.9013999999999999 Steps 69\n",
            "Episode: 57 Reward: 0.79 Average Reward: 0.898 Steps 46\n",
            "Episode: 58 Reward: 0.99 Average Reward: 0.9024000000000001 Steps 7\n",
            "Episode: 59 Reward: 0.99 Average Reward: 0.9022000000000001 Steps 10\n",
            "Episode: 60 Reward: 0.92 Average Reward: 0.902 Steps 36\n",
            "Episode: 61 Reward: 0.94 Average Reward: 0.9026000000000001 Steps 16\n",
            "Episode: 62 Reward: 0.95 Average Reward: 0.9046000000000001 Steps 14\n",
            "Episode: 63 Reward: 0.89 Average Reward: 0.9036000000000002 Steps 31\n",
            "Episode: 64 Reward: 0.95 Average Reward: 0.9030000000000001 Steps 16\n",
            "Episode: 65 Reward: 0.98 Average Reward: 0.9040000000000002 Steps 8\n",
            "Episode: 66 Reward: 1.0 Average Reward: 0.9039999999999999 Steps 6\n",
            "Episode: 67 Reward: 0.97 Average Reward: 0.9044 Steps 11\n",
            "Episode: 68 Reward: 0.97 Average Reward: 0.9076 Steps 11\n",
            "Episode: 69 Reward: 0.92 Average Reward: 0.9072000000000001 Steps 17\n",
            "Episode: 70 Reward: 0.96 Average Reward: 0.9082000000000001 Steps 18\n",
            "Episode: 71 Reward: 0.94 Average Reward: 0.911 Steps 20\n",
            "Episode: 72 Reward: 0.95 Average Reward: 0.9192 Steps 14\n",
            "Episode: 73 Reward: 0.99 Average Reward: 0.9234000000000002 Steps 10\n",
            "Episode: 74 Reward: 0.8699999999999999 Average Reward: 0.9236 Steps 32\n",
            "Episode: 75 Reward: 0.95 Average Reward: 0.927 Steps 15\n",
            "Episode: 76 Reward: 0.99 Average Reward: 0.927 Steps 8\n",
            "Episode: 77 Reward: 0.96 Average Reward: 0.927 Steps 13\n",
            "Episode: 78 Reward: 0.97 Average Reward: 0.9266 Steps 9\n",
            "Episode: 79 Reward: 0.89 Average Reward: 0.9296 Steps 21\n",
            "Episode: 80 Reward: 1.0 Average Reward: 0.9296 Steps 7\n",
            "Episode: 81 Reward: 0.99 Average Reward: 0.9342 Steps 8\n",
            "Episode: 82 Reward: 0.92 Average Reward: 0.9386 Steps 18\n",
            "Episode: 83 Reward: 0.9299999999999999 Average Reward: 0.9372 Steps 15\n",
            "Episode: 84 Reward: 0.93 Average Reward: 0.9366 Steps 18\n",
            "Episode: 85 Reward: 0.98 Average Reward: 0.9366 Steps 12\n",
            "Episode: 86 Reward: 0.94 Average Reward: 0.9371999999999998 Steps 15\n",
            "Episode: 87 Reward: 0.94 Average Reward: 0.9363999999999999 Steps 15\n",
            "Episode: 88 Reward: 0.96 Average Reward: 0.9358 Steps 12\n",
            "Episode: 89 Reward: 0.95 Average Reward: 0.9354 Steps 12\n",
            "Episode: 90 Reward: 0.96 Average Reward: 0.9348000000000001 Steps 11\n",
            "Episode: 91 Reward: 0.97 Average Reward: 0.9355999999999999 Steps 9\n",
            "Episode: 92 Reward: 0.84 Average Reward: 0.9342 Steps 23\n",
            "Episode: 93 Reward: 0.87 Average Reward: 0.9318 Steps 21\n",
            "Episode: 94 Reward: 0.9 Average Reward: 0.9313999999999999 Steps 17\n",
            "Episode: 95 Reward: 0.95 Average Reward: 0.9309999999999999 Steps 14\n",
            "Episode: 96 Reward: 0.87 Average Reward: 0.9289999999999999 Steps 23\n",
            "Episode: 97 Reward: 0.9 Average Reward: 0.9306 Steps 19\n",
            "Episode: 98 Reward: 0.97 Average Reward: 0.9311999999999999 Steps 11\n",
            "Episode: 99 Reward: 0.94 Average Reward: 0.9338 Steps 13\n",
            "Episode: 0 Reward: 0.83 Average Reward: 0.83 Steps 71\n",
            "Episode: 1 Reward: 0.79 Average Reward: 0.81 Steps 27\n",
            "Episode: 2 Reward: 1.0 Average Reward: 0.8733333333333334 Steps 4\n",
            "Episode: 3 Reward: 0.83 Average Reward: 0.8625 Steps 66\n",
            "Episode: 4 Reward: 0.8099999999999999 Average Reward: 0.852 Steps 79\n",
            "Episode: 5 Reward: 1.0 Average Reward: 0.8766666666666666 Steps 4\n",
            "Episode: 6 Reward: 0.76 Average Reward: 0.86 Steps 36\n",
            "Episode: 7 Reward: 0.99 Average Reward: 0.87625 Steps 8\n",
            "Episode: 8 Reward: 0.94 Average Reward: 0.8833333333333333 Steps 16\n",
            "Episode: 9 Reward: 0.7899999999999999 Average Reward: 0.8739999999999999 Steps 32\n",
            "Episode: 10 Reward: 0.98 Average Reward: 0.8836363636363636 Steps 14\n",
            "Episode: 11 Reward: 0.69 Average Reward: 0.8674999999999998 Steps 54\n",
            "Episode: 12 Reward: 0.94 Average Reward: 0.8730769230769229 Steps 16\n",
            "Episode: 13 Reward: 0.9 Average Reward: 0.8749999999999999 Steps 26\n",
            "Episode: 14 Reward: 1.0 Average Reward: 0.8833333333333332 Steps 7\n",
            "Episode: 15 Reward: 0.6799999999999999 Average Reward: 0.870625 Steps 53\n",
            "Episode: 16 Reward: 1.0 Average Reward: 0.878235294117647 Steps 4\n",
            "Episode: 17 Reward: 1.0 Average Reward: 0.885 Steps 4\n",
            "Episode: 18 Reward: 1.0 Average Reward: 0.8910526315789473 Steps 4\n",
            "Episode: 19 Reward: 1.0 Average Reward: 0.8965 Steps 4\n",
            "Episode: 20 Reward: 1.0 Average Reward: 0.9014285714285715 Steps 4\n",
            "Episode: 21 Reward: 1.0 Average Reward: 0.9059090909090909 Steps 4\n",
            "Episode: 22 Reward: 1.0 Average Reward: 0.91 Steps 4\n",
            "Episode: 23 Reward: 1.0 Average Reward: 0.91375 Steps 4\n",
            "Episode: 24 Reward: 0.54 Average Reward: 0.8987999999999999 Steps 69\n",
            "Episode: 25 Reward: 0.99 Average Reward: 0.9023076923076923 Steps 17\n",
            "Episode: 26 Reward: 0.88 Average Reward: 0.9014814814814813 Steps 38\n",
            "Episode: 27 Reward: 1.0 Average Reward: 0.9049999999999999 Steps 4\n",
            "Episode: 28 Reward: 1.0 Average Reward: 0.9082758620689654 Steps 5\n",
            "Episode: 29 Reward: 1.0 Average Reward: 0.9113333333333332 Steps 7\n",
            "Episode: 30 Reward: 1.0 Average Reward: 0.9141935483870967 Steps 8\n",
            "Episode: 31 Reward: 0.87 Average Reward: 0.9128125 Steps 29\n",
            "Episode: 32 Reward: 0.87 Average Reward: 0.9115151515151516 Steps 25\n",
            "Episode: 33 Reward: 1.0 Average Reward: 0.9141176470588236 Steps 4\n",
            "Episode: 34 Reward: 0.99 Average Reward: 0.9162857142857143 Steps 9\n",
            "Episode: 35 Reward: 1.0 Average Reward: 0.9186111111111112 Steps 4\n",
            "Episode: 36 Reward: 1.0 Average Reward: 0.9208108108108108 Steps 10\n",
            "Episode: 37 Reward: 0.98 Average Reward: 0.9223684210526315 Steps 7\n",
            "Episode: 38 Reward: 0.94 Average Reward: 0.9228205128205127 Steps 17\n",
            "Episode: 39 Reward: 0.9299999999999999 Average Reward: 0.9229999999999998 Steps 16\n",
            "Episode: 40 Reward: 0.8300000000000001 Average Reward: 0.920731707317073 Steps 33\n",
            "Episode: 41 Reward: 1.0 Average Reward: 0.9226190476190474 Steps 6\n",
            "Episode: 42 Reward: 1.0 Average Reward: 0.9244186046511627 Steps 4\n",
            "Episode: 43 Reward: 0.97 Average Reward: 0.9254545454545453 Steps 9\n",
            "Episode: 44 Reward: 0.96 Average Reward: 0.9262222222222221 Steps 9\n",
            "Episode: 45 Reward: 1.0 Average Reward: 0.9278260869565216 Steps 4\n",
            "Episode: 46 Reward: 1.0 Average Reward: 0.9293617021276595 Steps 4\n",
            "Episode: 47 Reward: 1.0 Average Reward: 0.9308333333333333 Steps 4\n",
            "Episode: 48 Reward: 0.88 Average Reward: 0.929795918367347 Steps 26\n",
            "Episode: 49 Reward: 0.9299999999999999 Average Reward: 0.9298000000000001 Steps 14\n",
            "Episode: 50 Reward: 1.0 Average Reward: 0.9331999999999999 Steps 4\n",
            "Episode: 51 Reward: 0.89 Average Reward: 0.9351999999999999 Steps 20\n",
            "Episode: 52 Reward: 1.0 Average Reward: 0.9351999999999998 Steps 4\n",
            "Episode: 53 Reward: 1.0 Average Reward: 0.9385999999999999 Steps 4\n",
            "Episode: 54 Reward: 1.0 Average Reward: 0.9423999999999999 Steps 4\n",
            "Episode: 55 Reward: 1.0 Average Reward: 0.9423999999999999 Steps 4\n",
            "Episode: 56 Reward: 0.92 Average Reward: 0.9456 Steps 15\n",
            "Episode: 57 Reward: 0.96 Average Reward: 0.945 Steps 9\n",
            "Episode: 58 Reward: 1.0 Average Reward: 0.9461999999999999 Steps 4\n",
            "Episode: 59 Reward: 0.73 Average Reward: 0.9449999999999998 Steps 37\n",
            "Episode: 60 Reward: 1.0 Average Reward: 0.9453999999999999 Steps 4\n",
            "Episode: 61 Reward: 1.0 Average Reward: 0.9516 Steps 4\n",
            "Episode: 62 Reward: 1.0 Average Reward: 0.9528 Steps 4\n",
            "Episode: 63 Reward: 1.0 Average Reward: 0.9547999999999999 Steps 4\n",
            "Episode: 64 Reward: 0.97 Average Reward: 0.9541999999999998 Steps 8\n",
            "Episode: 65 Reward: 0.98 Average Reward: 0.9601999999999999 Steps 7\n",
            "Episode: 66 Reward: 0.99 Average Reward: 0.96 Steps 6\n",
            "Episode: 67 Reward: 0.99 Average Reward: 0.9598000000000002 Steps 6\n",
            "Episode: 68 Reward: 1.0 Average Reward: 0.9598 Steps 4\n",
            "Episode: 69 Reward: 0.99 Average Reward: 0.9596000000000001 Steps 6\n",
            "Episode: 70 Reward: 0.99 Average Reward: 0.9594 Steps 6\n",
            "Episode: 71 Reward: 1.0 Average Reward: 0.9594000000000001 Steps 4\n",
            "Episode: 72 Reward: 1.0 Average Reward: 0.9594 Steps 4\n",
            "Episode: 73 Reward: 1.0 Average Reward: 0.9594 Steps 4\n",
            "Episode: 74 Reward: 0.99 Average Reward: 0.9684 Steps 6\n",
            "Episode: 75 Reward: 0.99 Average Reward: 0.9684 Steps 6\n",
            "Episode: 76 Reward: 1.0 Average Reward: 0.9708 Steps 5\n",
            "Episode: 77 Reward: 0.99 Average Reward: 0.9706000000000001 Steps 6\n",
            "Episode: 78 Reward: 0.99 Average Reward: 0.9704000000000002 Steps 6\n",
            "Episode: 79 Reward: 0.99 Average Reward: 0.9702000000000001 Steps 6\n",
            "Episode: 80 Reward: 0.99 Average Reward: 0.97 Steps 6\n",
            "Episode: 81 Reward: 0.99 Average Reward: 0.9724 Steps 6\n",
            "Episode: 82 Reward: 0.99 Average Reward: 0.9748000000000002 Steps 6\n",
            "Episode: 83 Reward: 0.99 Average Reward: 0.9746000000000001 Steps 6\n",
            "Episode: 84 Reward: 0.99 Average Reward: 0.9746000000000001 Steps 6\n",
            "Episode: 85 Reward: 0.99 Average Reward: 0.9743999999999999 Steps 6\n",
            "Episode: 86 Reward: 0.99 Average Reward: 0.9742000000000002 Steps 6\n",
            "Episode: 87 Reward: 0.99 Average Reward: 0.9744000000000002 Steps 6\n",
            "Episode: 88 Reward: 0.99 Average Reward: 0.9754000000000002 Steps 6\n",
            "Episode: 89 Reward: 0.99 Average Reward: 0.9766000000000002 Steps 6\n",
            "Episode: 90 Reward: 0.99 Average Reward: 0.9798000000000002 Steps 6\n",
            "Episode: 91 Reward: 0.99 Average Reward: 0.9796000000000002 Steps 6\n",
            "Episode: 92 Reward: 0.99 Average Reward: 0.9794000000000003 Steps 6\n",
            "Episode: 93 Reward: 0.99 Average Reward: 0.9798000000000002 Steps 6\n",
            "Episode: 94 Reward: 0.99 Average Reward: 0.9804000000000002 Steps 6\n",
            "Episode: 95 Reward: 0.99 Average Reward: 0.9802000000000003 Steps 6\n",
            "Episode: 96 Reward: 0.99 Average Reward: 0.9800000000000003 Steps 6\n",
            "Episode: 97 Reward: 0.99 Average Reward: 0.9798000000000002 Steps 6\n",
            "Episode: 98 Reward: 0.99 Average Reward: 0.9820000000000002 Steps 6\n",
            "Episode: 99 Reward: 0.99 Average Reward: 0.9832000000000002 Steps 6\n",
            "Episode: 0 Reward: -0.26 Average Reward: -0.26 Steps 100\n",
            "Episode: 1 Reward: -0.96 Average Reward: -0.61 Steps 100\n",
            "Episode: 2 Reward: -0.67 Average Reward: -0.63 Steps 100\n",
            "Episode: 3 Reward: -0.23 Average Reward: -0.53 Steps 100\n",
            "Episode: 4 Reward: 0.8 Average Reward: -0.264 Steps 51\n",
            "Episode: 5 Reward: 0.8200000000000001 Average Reward: -0.08333333333333333 Steps 60\n",
            "Episode: 6 Reward: 0.9299999999999999 Average Reward: 0.06142857142857142 Steps 32\n",
            "Episode: 7 Reward: 0.89 Average Reward: 0.16499999999999998 Steps 28\n",
            "Episode: 8 Reward: -0.39 Average Reward: 0.10333333333333332 Steps 100\n",
            "Episode: 9 Reward: 0.89 Average Reward: 0.182 Steps 60\n",
            "Episode: 10 Reward: 0.8 Average Reward: 0.2381818181818182 Steps 63\n",
            "Episode: 11 Reward: 0.9 Average Reward: 0.29333333333333333 Steps 22\n",
            "Episode: 12 Reward: 0.9299999999999999 Average Reward: 0.3423076923076923 Steps 18\n",
            "Episode: 13 Reward: 0.74 Average Reward: 0.3707142857142857 Steps 89\n",
            "Episode: 14 Reward: 0.85 Average Reward: 0.4026666666666667 Steps 55\n",
            "Episode: 15 Reward: 0.92 Average Reward: 0.435 Steps 34\n",
            "Episode: 16 Reward: 0.92 Average Reward: 0.46352941176470586 Steps 34\n",
            "Episode: 17 Reward: 0.96 Average Reward: 0.4911111111111111 Steps 12\n",
            "Episode: 18 Reward: 0.9299999999999999 Average Reward: 0.5142105263157895 Steps 19\n",
            "Episode: 19 Reward: 0.84 Average Reward: 0.5305 Steps 43\n",
            "Episode: 20 Reward: 0.98 Average Reward: 0.5519047619047619 Steps 8\n",
            "Episode: 21 Reward: 0.98 Average Reward: 0.5713636363636364 Steps 7\n",
            "Episode: 22 Reward: 0.96 Average Reward: 0.5882608695652174 Steps 14\n",
            "Episode: 23 Reward: 0.9 Average Reward: 0.60125 Steps 26\n",
            "Episode: 24 Reward: 0.97 Average Reward: 0.616 Steps 10\n",
            "Episode: 25 Reward: 0.91 Average Reward: 0.6273076923076922 Steps 25\n",
            "Episode: 26 Reward: 0.9 Average Reward: 0.6374074074074073 Steps 19\n",
            "Episode: 27 Reward: 0.96 Average Reward: 0.6489285714285714 Steps 13\n",
            "Episode: 28 Reward: 0.88 Average Reward: 0.6568965517241379 Steps 20\n",
            "Episode: 29 Reward: 0.97 Average Reward: 0.6673333333333332 Steps 11\n",
            "Episode: 30 Reward: 0.98 Average Reward: 0.6774193548387095 Steps 8\n",
            "Episode: 31 Reward: 0.91 Average Reward: 0.6846875 Steps 18\n",
            "Episode: 32 Reward: 0.98 Average Reward: 0.6936363636363636 Steps 8\n",
            "Episode: 33 Reward: 0.94 Average Reward: 0.7008823529411765 Steps 14\n",
            "Episode: 34 Reward: 0.71 Average Reward: 0.7011428571428572 Steps 37\n",
            "Episode: 35 Reward: 0.8999999999999999 Average Reward: 0.7066666666666667 Steps 16\n",
            "Episode: 36 Reward: 0.99 Average Reward: 0.7143243243243244 Steps 5\n",
            "Episode: 37 Reward: 0.85 Average Reward: 0.7178947368421053 Steps 21\n",
            "Episode: 38 Reward: 0.98 Average Reward: 0.7246153846153847 Steps 6\n",
            "Episode: 39 Reward: 0.99 Average Reward: 0.73125 Steps 5\n",
            "Episode: 40 Reward: -0.96 Average Reward: 0.69 Steps 100\n",
            "Episode: 41 Reward: 1.0 Average Reward: 0.6973809523809523 Steps 4\n",
            "Episode: 42 Reward: 0.99 Average Reward: 0.7041860465116279 Steps 5\n",
            "Episode: 43 Reward: 0.99 Average Reward: 0.7106818181818181 Steps 5\n",
            "Episode: 44 Reward: -0.96 Average Reward: 0.6735555555555555 Steps 100\n",
            "Episode: 45 Reward: 0.62 Average Reward: 0.672391304347826 Steps 44\n",
            "Episode: 46 Reward: -0.96 Average Reward: 0.637659574468085 Steps 100\n",
            "Episode: 47 Reward: 0.13000000000000012 Average Reward: 0.6270833333333333 Steps 93\n",
            "Episode: 48 Reward: 0.98 Average Reward: 0.6342857142857142 Steps 7\n",
            "Episode: 49 Reward: 0.91 Average Reward: 0.6397999999999999 Steps 15\n",
            "Episode: 50 Reward: 1.0 Average Reward: 0.6649999999999998 Steps 4\n",
            "Episode: 51 Reward: 1.0 Average Reward: 0.7041999999999998 Steps 4\n",
            "Episode: 52 Reward: 0.94 Average Reward: 0.7363999999999998 Steps 11\n",
            "Episode: 53 Reward: 0.98 Average Reward: 0.7605999999999998 Steps 7\n",
            "Episode: 54 Reward: 0.98 Average Reward: 0.7641999999999999 Steps 8\n",
            "Episode: 55 Reward: 0.99 Average Reward: 0.7676 Steps 5\n",
            "Episode: 56 Reward: 0.9299999999999999 Average Reward: 0.7676000000000001 Steps 12\n",
            "Episode: 57 Reward: 0.9 Average Reward: 0.7678 Steps 23\n",
            "Episode: 58 Reward: 1.0 Average Reward: 0.7955999999999999 Steps 4\n",
            "Episode: 59 Reward: 0.88 Average Reward: 0.7954000000000001 Steps 19\n",
            "Episode: 60 Reward: 1.0 Average Reward: 0.7994000000000001 Steps 4\n",
            "Episode: 61 Reward: 0.91 Average Reward: 0.7996 Steps 18\n",
            "Episode: 62 Reward: 0.99 Average Reward: 0.8008 Steps 5\n",
            "Episode: 63 Reward: 1.0 Average Reward: 0.806 Steps 4\n",
            "Episode: 64 Reward: 0.91 Average Reward: 0.8072 Steps 14\n",
            "Episode: 65 Reward: 0.9199999999999999 Average Reward: 0.8072 Steps 16\n",
            "Episode: 66 Reward: 1.0 Average Reward: 0.8088 Steps 4\n",
            "Episode: 67 Reward: 0.98 Average Reward: 0.8092 Steps 7\n",
            "Episode: 68 Reward: 0.96 Average Reward: 0.8097999999999999 Steps 11\n",
            "Episode: 69 Reward: 0.96 Average Reward: 0.8122 Steps 9\n",
            "Episode: 70 Reward: 1.0 Average Reward: 0.8126000000000001 Steps 4\n",
            "Episode: 71 Reward: 0.99 Average Reward: 0.8128 Steps 5\n",
            "Episode: 72 Reward: 1.0 Average Reward: 0.8136 Steps 4\n",
            "Episode: 73 Reward: 1.0 Average Reward: 0.8156 Steps 4\n",
            "Episode: 74 Reward: 0.95 Average Reward: 0.8152000000000001 Steps 10\n",
            "Episode: 75 Reward: 1.0 Average Reward: 0.8170000000000001 Steps 4\n",
            "Episode: 76 Reward: 1.0 Average Reward: 0.8190000000000001 Steps 4\n",
            "Episode: 77 Reward: 0.5099999999999999 Average Reward: 0.8099999999999998 Steps 56\n",
            "Episode: 78 Reward: 0.99 Average Reward: 0.8122 Steps 5\n",
            "Episode: 79 Reward: 1.0 Average Reward: 0.8128000000000002 Steps 4\n",
            "Episode: 80 Reward: 1.0 Average Reward: 0.8132 Steps 4\n",
            "Episode: 81 Reward: 1.0 Average Reward: 0.815 Steps 4\n",
            "Episode: 82 Reward: 1.0 Average Reward: 0.8154 Steps 4\n",
            "Episode: 83 Reward: 1.0 Average Reward: 0.8166 Steps 4\n",
            "Episode: 84 Reward: 1.0 Average Reward: 0.8224000000000001 Steps 4\n",
            "Episode: 85 Reward: 0.99 Average Reward: 0.8242 Steps 5\n",
            "Episode: 86 Reward: 1.0 Average Reward: 0.8244000000000001 Steps 4\n",
            "Episode: 87 Reward: 1.0 Average Reward: 0.8274000000000001 Steps 4\n",
            "Episode: 88 Reward: 1.0 Average Reward: 0.8278 Steps 4\n",
            "Episode: 89 Reward: 1.0 Average Reward: 0.828 Steps 4\n",
            "Episode: 90 Reward: 1.0 Average Reward: 0.8672 Steps 4\n",
            "Episode: 91 Reward: 1.0 Average Reward: 0.8672 Steps 4\n",
            "Episode: 92 Reward: 1.0 Average Reward: 0.8674 Steps 4\n",
            "Episode: 93 Reward: 1.0 Average Reward: 0.8675999999999999 Steps 4\n",
            "Episode: 94 Reward: 1.0 Average Reward: 0.9067999999999999 Steps 4\n",
            "Episode: 95 Reward: 1.0 Average Reward: 0.9144 Steps 4\n",
            "Episode: 96 Reward: 1.0 Average Reward: 0.9536 Steps 4\n",
            "Episode: 97 Reward: 1.0 Average Reward: 0.971 Steps 4\n",
            "Episode: 98 Reward: 1.0 Average Reward: 0.9713999999999998 Steps 4\n",
            "Episode: 99 Reward: 1.0 Average Reward: 0.9732 Steps 4\n",
            "Episode: 0 Reward: -0.14 Average Reward: -0.14 Steps 100\n",
            "Episode: 1 Reward: -0.9999999999999999 Average Reward: -0.57 Steps 100\n",
            "Episode: 2 Reward: -0.39 Average Reward: -0.5099999999999999 Steps 100\n",
            "Episode: 3 Reward: -0.36000000000000004 Average Reward: -0.4725 Steps 100\n",
            "Episode: 4 Reward: 0.86 Average Reward: -0.20599999999999996 Steps 26\n",
            "Episode: 5 Reward: 0.99 Average Reward: -0.006666666666666636 Steps 13\n",
            "Episode: 6 Reward: 0.94 Average Reward: 0.1285714285714286 Steps 18\n",
            "Episode: 7 Reward: -0.98 Average Reward: -0.009999999999999981 Steps 100\n",
            "Episode: 8 Reward: 0.92 Average Reward: 0.09333333333333335 Steps 37\n",
            "Episode: 9 Reward: -0.53 Average Reward: 0.031000000000000017 Steps 100\n",
            "Episode: 10 Reward: 0.92 Average Reward: 0.11181818181818183 Steps 21\n",
            "Episode: 11 Reward: 0.98 Average Reward: 0.18416666666666667 Steps 14\n",
            "Episode: 12 Reward: 0.85 Average Reward: 0.2353846153846154 Steps 40\n",
            "Episode: 13 Reward: 0.87 Average Reward: 0.28071428571428575 Steps 35\n",
            "Episode: 14 Reward: 0.9 Average Reward: 0.322 Steps 22\n",
            "Episode: 15 Reward: 0.99 Average Reward: 0.36375 Steps 13\n",
            "Episode: 16 Reward: 0.96 Average Reward: 0.39882352941176474 Steps 16\n",
            "Episode: 17 Reward: 0.78 Average Reward: 0.42000000000000004 Steps 34\n",
            "Episode: 18 Reward: 0.9299999999999999 Average Reward: 0.44684210526315793 Steps 19\n",
            "Episode: 19 Reward: 0.96 Average Reward: 0.4725 Steps 16\n",
            "Episode: 20 Reward: 1.0 Average Reward: 0.49761904761904757 Steps 8\n",
            "Episode: 21 Reward: 0.9299999999999999 Average Reward: 0.5172727272727272 Steps 23\n",
            "Episode: 22 Reward: 0.95 Average Reward: 0.5360869565217391 Steps 19\n",
            "Episode: 23 Reward: 1.0 Average Reward: 0.5554166666666668 Steps 10\n",
            "Episode: 24 Reward: 0.97 Average Reward: 0.5720000000000001 Steps 15\n",
            "Episode: 25 Reward: 0.9 Average Reward: 0.5846153846153848 Steps 21\n",
            "Episode: 26 Reward: 0.8799999999999999 Average Reward: 0.5955555555555556 Steps 24\n",
            "Episode: 27 Reward: 0.99 Average Reward: 0.6096428571428572 Steps 13\n",
            "Episode: 28 Reward: 0.94 Average Reward: 0.6210344827586207 Steps 18\n",
            "Episode: 29 Reward: 0.98 Average Reward: 0.6330000000000001 Steps 14\n",
            "Episode: 30 Reward: 0.78 Average Reward: 0.637741935483871 Steps 34\n",
            "Episode: 31 Reward: 1.0 Average Reward: 0.6490625 Steps 12\n",
            "Episode: 32 Reward: 0.96 Average Reward: 0.6584848484848485 Steps 16\n",
            "Episode: 33 Reward: 0.97 Average Reward: 0.6676470588235294 Steps 15\n",
            "Episode: 34 Reward: 0.99 Average Reward: 0.6768571428571428 Steps 13\n",
            "Episode: 35 Reward: 0.97 Average Reward: 0.6849999999999999 Steps 15\n",
            "Episode: 36 Reward: 0.96 Average Reward: 0.6924324324324324 Steps 16\n",
            "Episode: 37 Reward: 0.98 Average Reward: 0.7 Steps 14\n",
            "Episode: 38 Reward: 0.97 Average Reward: 0.7069230769230769 Steps 15\n",
            "Episode: 39 Reward: 1.0 Average Reward: 0.71425 Steps 12\n",
            "Episode: 40 Reward: 1.0 Average Reward: 0.7212195121951219 Steps 12\n",
            "Episode: 41 Reward: 0.89 Average Reward: 0.7252380952380952 Steps 23\n",
            "Episode: 42 Reward: 0.97 Average Reward: 0.7309302325581395 Steps 15\n",
            "Episode: 43 Reward: 0.61 Average Reward: 0.7281818181818182 Steps 51\n",
            "Episode: 44 Reward: 0.73 Average Reward: 0.7282222222222221 Steps 39\n",
            "Episode: 45 Reward: 0.89 Average Reward: 0.7317391304347826 Steps 23\n",
            "Episode: 46 Reward: 0.99 Average Reward: 0.7372340425531915 Steps 12\n",
            "Episode: 47 Reward: 0.98 Average Reward: 0.7422916666666666 Steps 14\n",
            "Episode: 48 Reward: 0.96 Average Reward: 0.746734693877551 Steps 21\n",
            "Episode: 49 Reward: 0.9299999999999999 Average Reward: 0.7504 Steps 30\n",
            "Episode: 50 Reward: 0.96 Average Reward: 0.7724000000000001 Steps 21\n",
            "Episode: 51 Reward: 0.94 Average Reward: 0.8111999999999999 Steps 18\n",
            "Episode: 52 Reward: 0.97 Average Reward: 0.8384 Steps 17\n",
            "Episode: 53 Reward: 0.83 Average Reward: 0.8622 Steps 24\n",
            "Episode: 54 Reward: 0.93 Average Reward: 0.8636 Steps 18\n",
            "Episode: 55 Reward: 0.96 Average Reward: 0.863 Steps 18\n",
            "Episode: 56 Reward: 0.94 Average Reward: 0.863 Steps 23\n",
            "Episode: 57 Reward: 0.95 Average Reward: 0.9016 Steps 17\n",
            "Episode: 58 Reward: 0.96 Average Reward: 0.9024 Steps 13\n",
            "Episode: 59 Reward: 0.98 Average Reward: 0.9325999999999999 Steps 14\n",
            "Episode: 60 Reward: 0.99 Average Reward: 0.9339999999999999 Steps 15\n",
            "Episode: 61 Reward: 0.85 Average Reward: 0.9314000000000001 Steps 27\n",
            "Episode: 62 Reward: 0.8799999999999999 Average Reward: 0.932 Steps 24\n",
            "Episode: 63 Reward: 0.92 Average Reward: 0.9329999999999999 Steps 22\n",
            "Episode: 64 Reward: 0.98 Average Reward: 0.9346 Steps 13\n",
            "Episode: 65 Reward: 0.9299999999999999 Average Reward: 0.9333999999999999 Steps 17\n",
            "Episode: 66 Reward: 0.99 Average Reward: 0.934 Steps 14\n",
            "Episode: 67 Reward: 0.94 Average Reward: 0.9372 Steps 20\n",
            "Episode: 68 Reward: 0.91 Average Reward: 0.9368 Steps 21\n",
            "Episode: 69 Reward: 0.97 Average Reward: 0.9369999999999998 Steps 16\n",
            "Episode: 70 Reward: 0.95 Average Reward: 0.9359999999999999 Steps 21\n",
            "Episode: 71 Reward: 0.96 Average Reward: 0.9366000000000001 Steps 15\n",
            "Episode: 72 Reward: 0.92 Average Reward: 0.936 Steps 21\n",
            "Episode: 73 Reward: 0.9299999999999999 Average Reward: 0.9346000000000001 Steps 23\n",
            "Episode: 74 Reward: 0.95 Average Reward: 0.9342 Steps 24\n",
            "Episode: 75 Reward: 0.96 Average Reward: 0.9354 Steps 16\n",
            "Episode: 76 Reward: 0.98 Average Reward: 0.9373999999999999 Steps 20\n",
            "Episode: 77 Reward: 0.91 Average Reward: 0.9357999999999999 Steps 21\n",
            "Episode: 78 Reward: 0.98 Average Reward: 0.9365999999999999 Steps 14\n",
            "Episode: 79 Reward: 0.93 Average Reward: 0.9356 Steps 17\n",
            "Episode: 80 Reward: 0.92 Average Reward: 0.9384 Steps 20\n",
            "Episode: 81 Reward: 0.99 Average Reward: 0.9382 Steps 13\n",
            "Episode: 82 Reward: 0.89 Average Reward: 0.9368 Steps 25\n",
            "Episode: 83 Reward: 0.96 Average Reward: 0.9366000000000001 Steps 16\n",
            "Episode: 84 Reward: 0.99 Average Reward: 0.9366 Steps 13\n",
            "Episode: 85 Reward: 0.9299999999999999 Average Reward: 0.9358000000000001 Steps 19\n",
            "Episode: 86 Reward: 0.98 Average Reward: 0.9361999999999999 Steps 14\n",
            "Episode: 87 Reward: 0.95 Average Reward: 0.9355999999999999 Steps 15\n",
            "Episode: 88 Reward: 0.99 Average Reward: 0.936 Steps 13\n",
            "Episode: 89 Reward: 1.0 Average Reward: 0.936 Steps 12\n",
            "Episode: 90 Reward: 0.89 Average Reward: 0.9338 Steps 23\n",
            "Episode: 91 Reward: 0.89 Average Reward: 0.9338 Steps 23\n",
            "Episode: 92 Reward: 0.89 Average Reward: 0.9322 Steps 23\n",
            "Episode: 93 Reward: 0.79 Average Reward: 0.9358 Steps 33\n",
            "Episode: 94 Reward: 0.99 Average Reward: 0.941 Steps 13\n",
            "Episode: 95 Reward: 0.92 Average Reward: 0.9416000000000001 Steps 20\n",
            "Episode: 96 Reward: 0.99 Average Reward: 0.9416000000000001 Steps 13\n",
            "Episode: 97 Reward: 0.99 Average Reward: 0.9418000000000001 Steps 13\n",
            "Episode: 98 Reward: 0.99 Average Reward: 0.9423999999999999 Steps 13\n",
            "Episode: 99 Reward: 0.95 Average Reward: 0.9428000000000002 Steps 17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on MiniHack-Eat-v0"
      ],
      "metadata": {
        "id": "uboJUL1snzmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eat_rewards = []\n",
        "moves = tuple(nethack.CompassDirection)\n",
        "# all_moves = moves + (nethack.Command.EAT,)\n",
        "env = gym.make(\"MiniHack-Eat-v0\", observation_keys=[\"glyphs\",\"pixel\",\"message\"],\n",
        "               actions = moves + (nethack.Command.EAT,), max_episode_steps=1000)\n",
        "\n",
        "seeds = getSeeds(5)\n",
        "for seed in seeds:\n",
        "  act_critic_model = ActorCritic3(h_size=512, a_size=env.action_space.n)\n",
        "  policy, model_rewards = run(env=env,model = act_critic_model, seed = seed )\n",
        "  eat_rewards.append(model_rewards)\n",
        "\n",
        "plot_results(\"MiniHack-Eat-v0\",eat_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtzm8C-NZe32",
        "outputId": "aab260ff-e5d8-4c96-83d5-8afa96ad441c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Reward: 0.9 Average Reward: 0.9 Steps 34\n",
            "Episode: 1 Reward: -2.48 Average Reward: -0.79 Steps 250\n",
            "Episode: 2 Reward: -2.48 Average Reward: -1.3533333333333335 Steps 250\n",
            "Episode: 3 Reward: 0.63 Average Reward: -0.8575000000000002 Steps 89\n",
            "Episode: 4 Reward: -0.8300000000000001 Average Reward: -0.8520000000000001 Steps 250\n",
            "Episode: 5 Reward: -0.81 Average Reward: -0.8450000000000001 Steps 250\n",
            "Episode: 6 Reward: 0.3299999999999999 Average Reward: -0.6771428571428572 Steps 177\n",
            "Episode: 7 Reward: 0.81 Average Reward: -0.4912500000000001 Steps 42\n",
            "Episode: 8 Reward: -1.1500000000000001 Average Reward: -0.5644444444444445 Steps 250\n",
            "Episode: 9 Reward: -0.87 Average Reward: -0.5950000000000001 Steps 250\n",
            "Episode: 10 Reward: 0.2499999999999999 Average Reward: -0.5181818181818183 Steps 205\n",
            "Episode: 11 Reward: -1.1400000000000001 Average Reward: -0.5700000000000002 Steps 250\n",
            "Episode: 12 Reward: 0.5699999999999998 Average Reward: -0.48230769230769244 Steps 112\n",
            "Episode: 13 Reward: -1.03 Average Reward: -0.5214285714285716 Steps 250\n",
            "Episode: 14 Reward: 0.5199999999999999 Average Reward: -0.4520000000000001 Steps 117\n",
            "Episode: 15 Reward: -1.06 Average Reward: -0.49000000000000005 Steps 250\n",
            "Episode: 16 Reward: -0.8300000000000001 Average Reward: -0.5100000000000001 Steps 250\n",
            "Episode: 17 Reward: 0.1299999999999999 Average Reward: -0.4744444444444446 Steps 231\n",
            "Episode: 18 Reward: -0.9500000000000001 Average Reward: -0.4994736842105264 Steps 250\n",
            "Episode: 19 Reward: -1.07 Average Reward: -0.5280000000000001 Steps 250\n",
            "Episode: 20 Reward: 0.6099999999999999 Average Reward: -0.47380952380952396 Steps 96\n",
            "Episode: 21 Reward: 0.6 Average Reward: -0.42500000000000016 Steps 83\n",
            "Episode: 22 Reward: -1.02 Average Reward: -0.4508695652173914 Steps 250\n",
            "Episode: 23 Reward: -1.03 Average Reward: -0.4750000000000001 Steps 250\n",
            "Episode: 24 Reward: -0.94 Average Reward: -0.49360000000000004 Steps 250\n",
            "Episode: 25 Reward: 0.94 Average Reward: -0.43846153846153857 Steps 18\n",
            "Episode: 26 Reward: 0.9299999999999999 Average Reward: -0.3877777777777779 Steps 21\n",
            "Episode: 27 Reward: 0.78 Average Reward: -0.3460714285714287 Steps 74\n",
            "Episode: 28 Reward: 0.95 Average Reward: -0.30137931034482773 Steps 15\n",
            "Episode: 29 Reward: -0.8400000000000001 Average Reward: -0.31933333333333347 Steps 250\n",
            "Episode: 30 Reward: -0.9800000000000001 Average Reward: -0.3406451612903227 Steps 250\n",
            "Episode: 31 Reward: 0.18999999999999995 Average Reward: -0.32406250000000003 Steps 211\n",
            "Episode: 32 Reward: 0.48999999999999994 Average Reward: -0.29939393939393943 Steps 165\n",
            "Episode: 33 Reward: -0.77 Average Reward: -0.31323529411764706 Steps 250\n",
            "Episode: 34 Reward: -0.87 Average Reward: -0.3291428571428571 Steps 250\n",
            "Episode: 35 Reward: -0.8200000000000001 Average Reward: -0.3427777777777778 Steps 250\n",
            "Episode: 36 Reward: -0.9400000000000001 Average Reward: -0.3589189189189189 Steps 250\n",
            "Episode: 37 Reward: -0.9400000000000001 Average Reward: -0.37421052631578944 Steps 250\n",
            "Episode: 38 Reward: -0.9 Average Reward: -0.38769230769230767 Steps 250\n",
            "Episode: 39 Reward: 0.24 Average Reward: -0.372 Steps 182\n",
            "Episode: 40 Reward: -0.8300000000000001 Average Reward: -0.3831707317073171 Steps 250\n",
            "Episode: 41 Reward: -0.8200000000000001 Average Reward: -0.39357142857142857 Steps 250\n",
            "Episode: 42 Reward: -0.8300000000000001 Average Reward: -0.4037209302325581 Steps 250\n",
            "Episode: 43 Reward: -1.03 Average Reward: -0.41795454545454547 Steps 250\n",
            "Episode: 44 Reward: -0.92 Average Reward: -0.42911111111111117 Steps 250\n",
            "Episode: 45 Reward: -0.88 Average Reward: -0.4389130434782609 Steps 250\n",
            "Episode: 46 Reward: 0.019999999999999907 Average Reward: -0.4291489361702128 Steps 243\n",
            "Episode: 47 Reward: 0.15000000000000002 Average Reward: -0.4170833333333334 Steps 235\n",
            "Episode: 48 Reward: 0.62 Average Reward: -0.3959183673469388 Steps 68\n",
            "Episode: 49 Reward: 0.43999999999999995 Average Reward: -0.37920000000000004 Steps 87\n",
            "Episode: 50 Reward: -0.9700000000000001 Average Reward: -0.41659999999999997 Steps 250\n",
            "Episode: 51 Reward: 0.40999999999999986 Average Reward: -0.3588 Steps 200\n",
            "Episode: 52 Reward: -0.8800000000000001 Average Reward: -0.3268 Steps 250\n",
            "Episode: 53 Reward: -0.8200000000000001 Average Reward: -0.35580000000000006 Steps 250\n",
            "Episode: 54 Reward: 0.27 Average Reward: -0.33380000000000004 Steps 186\n",
            "Episode: 55 Reward: -0.71 Average Reward: -0.3318000000000001 Steps 250\n",
            "Episode: 56 Reward: 0.08999999999999997 Average Reward: -0.3366 Steps 233\n",
            "Episode: 57 Reward: -0.7100000000000002 Average Reward: -0.3670000000000001 Steps 250\n",
            "Episode: 58 Reward: 0.41999999999999993 Average Reward: -0.3356 Steps 151\n",
            "Episode: 59 Reward: -1.04 Average Reward: -0.33900000000000013 Steps 250\n",
            "Episode: 60 Reward: -0.95 Average Reward: -0.36300000000000004 Steps 250\n",
            "Episode: 61 Reward: -1.07 Average Reward: -0.36160000000000003 Steps 250\n",
            "Episode: 62 Reward: 0.19999999999999984 Average Reward: -0.36900000000000005 Steps 181\n",
            "Episode: 63 Reward: 0.29999999999999993 Average Reward: -0.3424000000000001 Steps 148\n",
            "Episode: 64 Reward: 0.17999999999999994 Average Reward: -0.34920000000000007 Steps 154\n",
            "Episode: 65 Reward: -0.8800000000000001 Average Reward: -0.3456000000000001 Steps 250\n",
            "Episode: 66 Reward: -1.04 Average Reward: -0.34980000000000006 Steps 250\n",
            "Episode: 67 Reward: -1.23 Average Reward: -0.377 Steps 250\n",
            "Episode: 68 Reward: -0.97 Average Reward: -0.3774 Steps 250\n",
            "Episode: 69 Reward: -0.2499999999999999 Average Reward: -0.361 Steps 183\n",
            "Episode: 70 Reward: 0.63 Average Reward: -0.3606000000000001 Steps 42\n",
            "Episode: 71 Reward: -1.07 Average Reward: -0.39400000000000013 Steps 250\n",
            "Episode: 72 Reward: 0.37 Average Reward: -0.36620000000000014 Steps 153\n",
            "Episode: 73 Reward: -0.97 Average Reward: -0.365 Steps 250\n",
            "Episode: 74 Reward: 0.13999999999999996 Average Reward: -0.34340000000000004 Steps 204\n",
            "Episode: 75 Reward: -0.040000000000000036 Average Reward: -0.36300000000000004 Steps 222\n",
            "Episode: 76 Reward: 0.92 Average Reward: -0.3632 Steps 21\n",
            "Episode: 77 Reward: -1.08 Average Reward: -0.4004000000000001 Steps 250\n",
            "Episode: 78 Reward: -1.13 Average Reward: -0.4420000000000001 Steps 250\n",
            "Episode: 79 Reward: 0.06999999999999995 Average Reward: -0.4238 Steps 166\n",
            "Episode: 80 Reward: -0.9000000000000001 Average Reward: -0.42219999999999996 Steps 250\n",
            "Episode: 81 Reward: -0.28 Average Reward: -0.43160000000000004 Steps 244\n",
            "Episode: 82 Reward: -1.12 Average Reward: -0.4638000000000001 Steps 250\n",
            "Episode: 83 Reward: -0.9500000000000001 Average Reward: -0.4674000000000001 Steps 250\n",
            "Episode: 84 Reward: 0.15999999999999998 Average Reward: -0.4468000000000001 Steps 163\n",
            "Episode: 85 Reward: -0.98 Average Reward: -0.45000000000000007 Steps 250\n",
            "Episode: 86 Reward: 0.8200000000000001 Average Reward: -0.41480000000000006 Steps 42\n",
            "Episode: 87 Reward: -0.8500000000000001 Average Reward: -0.41300000000000003 Steps 250\n",
            "Episode: 88 Reward: -0.8400000000000001 Average Reward: -0.4118 Steps 250\n",
            "Episode: 89 Reward: 0.5599999999999999 Average Reward: -0.40540000000000004 Steps 104\n",
            "Episode: 90 Reward: -1.2000000000000002 Average Reward: -0.4128 Steps 250\n",
            "Episode: 91 Reward: -0.02000000000000013 Average Reward: -0.3968 Steps 170\n",
            "Episode: 92 Reward: -0.8600000000000001 Average Reward: -0.39740000000000003 Steps 250\n",
            "Episode: 93 Reward: 0.12 Average Reward: -0.37439999999999996 Steps 216\n",
            "Episode: 94 Reward: 0.18999999999999995 Average Reward: -0.3522 Steps 196\n",
            "Episode: 95 Reward: 0.32999999999999996 Average Reward: -0.32800000000000007 Steps 185\n",
            "Episode: 96 Reward: 0.43999999999999995 Average Reward: -0.3196000000000001 Steps 144\n",
            "Episode: 97 Reward: 0.2999999999999999 Average Reward: -0.3166 Steps 145\n",
            "Episode: 98 Reward: 0.95 Average Reward: -0.31000000000000005 Steps 9\n",
            "Episode: 99 Reward: 0.79 Average Reward: -0.30300000000000005 Steps 53\n",
            "Episode: 0 Reward: -0.8800000000000001 Average Reward: -0.8800000000000001 Steps 250\n",
            "Episode: 1 Reward: -2.39 Average Reward: -1.6350000000000002 Steps 250\n",
            "Episode: 2 Reward: -0.8400000000000001 Average Reward: -1.37 Steps 250\n",
            "Episode: 3 Reward: -0.8500000000000001 Average Reward: -1.2400000000000002 Steps 250\n",
            "Episode: 4 Reward: 0.88 Average Reward: -0.8160000000000002 Steps 46\n",
            "Episode: 5 Reward: -0.8800000000000001 Average Reward: -0.8266666666666668 Steps 250\n",
            "Episode: 6 Reward: -0.99 Average Reward: -0.8500000000000002 Steps 250\n",
            "Episode: 7 Reward: -0.93 Average Reward: -0.8600000000000001 Steps 250\n",
            "Episode: 8 Reward: -0.8 Average Reward: -0.8533333333333334 Steps 250\n",
            "Episode: 9 Reward: 0.71 Average Reward: -0.6970000000000001 Steps 60\n",
            "Episode: 10 Reward: 0.3799999999999998 Average Reward: -0.5990909090909091 Steps 152\n",
            "Episode: 11 Reward: -1.2100000000000002 Average Reward: -0.65 Steps 250\n",
            "Episode: 12 Reward: 0.6599999999999999 Average Reward: -0.5492307692307693 Steps 126\n",
            "Episode: 13 Reward: -0.7000000000000001 Average Reward: -0.56 Steps 250\n",
            "Episode: 14 Reward: -0.6400000000000001 Average Reward: -0.5653333333333334 Steps 250\n",
            "Episode: 15 Reward: -0.73 Average Reward: -0.575625 Steps 250\n",
            "Episode: 16 Reward: -0.8400000000000001 Average Reward: -0.5911764705882353 Steps 250\n",
            "Episode: 17 Reward: 0.5099999999999999 Average Reward: -0.53 Steps 112\n",
            "Episode: 18 Reward: -0.8 Average Reward: -0.5442105263157896 Steps 250\n",
            "Episode: 19 Reward: 0.6599999999999999 Average Reward: -0.4840000000000001 Steps 56\n",
            "Episode: 20 Reward: 0.38 Average Reward: -0.4428571428571429 Steps 186\n",
            "Episode: 21 Reward: 0.41999999999999993 Average Reward: -0.4036363636363637 Steps 169\n",
            "Episode: 22 Reward: 0.63 Average Reward: -0.358695652173913 Steps 73\n",
            "Episode: 23 Reward: 0.2799999999999999 Average Reward: -0.33208333333333345 Steps 168\n",
            "Episode: 24 Reward: 0.4099999999999999 Average Reward: -0.3024000000000001 Steps 119\n",
            "Episode: 25 Reward: 0.5499999999999999 Average Reward: -0.2696153846153847 Steps 81\n",
            "Episode: 26 Reward: 0.3899999999999999 Average Reward: -0.24518518518518528 Steps 166\n",
            "Episode: 27 Reward: 0.91 Average Reward: -0.20392857142857151 Steps 10\n",
            "Episode: 28 Reward: -0.88 Average Reward: -0.2272413793103449 Steps 250\n",
            "Episode: 29 Reward: -0.8200000000000001 Average Reward: -0.24700000000000008 Steps 250\n",
            "Episode: 30 Reward: 0.79 Average Reward: -0.2135483870967743 Steps 57\n",
            "Episode: 31 Reward: 0.43999999999999995 Average Reward: -0.19312500000000007 Steps 136\n",
            "Episode: 32 Reward: 0.25999999999999984 Average Reward: -0.17939393939393947 Steps 216\n",
            "Episode: 33 Reward: -0.8 Average Reward: -0.19764705882352948 Steps 250\n",
            "Episode: 34 Reward: -0.81 Average Reward: -0.21514285714285722 Steps 250\n",
            "Episode: 35 Reward: -0.79 Average Reward: -0.2311111111111112 Steps 250\n",
            "Episode: 36 Reward: -0.8200000000000001 Average Reward: -0.24702702702702714 Steps 250\n",
            "Episode: 37 Reward: 0.18 Average Reward: -0.23578947368421063 Steps 249\n",
            "Episode: 38 Reward: -0.67 Average Reward: -0.24692307692307702 Steps 250\n",
            "Episode: 39 Reward: -0.81 Average Reward: -0.26100000000000007 Steps 250\n",
            "Episode: 40 Reward: -0.7900000000000001 Average Reward: -0.2739024390243903 Steps 250\n",
            "Episode: 41 Reward: -0.75 Average Reward: -0.28523809523809535 Steps 250\n",
            "Episode: 42 Reward: -0.81 Average Reward: -0.2974418604651164 Steps 250\n",
            "Episode: 43 Reward: -0.6499999999999999 Average Reward: -0.3054545454545456 Steps 250\n",
            "Episode: 44 Reward: -0.67 Average Reward: -0.31355555555555564 Steps 250\n",
            "Episode: 45 Reward: -0.8400000000000001 Average Reward: -0.3250000000000001 Steps 250\n",
            "Episode: 46 Reward: 0.6199999999999999 Average Reward: -0.3048936170212767 Steps 95\n",
            "Episode: 47 Reward: -0.72 Average Reward: -0.3135416666666667 Steps 250\n",
            "Episode: 48 Reward: -0.81 Average Reward: -0.32367346938775515 Steps 250\n",
            "Episode: 49 Reward: -0.72 Average Reward: -0.33160000000000006 Steps 250\n",
            "Episode: 50 Reward: -0.7100000000000001 Average Reward: -0.32820000000000005 Steps 250\n",
            "Episode: 51 Reward: -0.69 Average Reward: -0.2942000000000001 Steps 250\n",
            "Episode: 52 Reward: -0.8 Average Reward: -0.29340000000000005 Steps 250\n",
            "Episode: 53 Reward: -0.71 Average Reward: -0.2906000000000001 Steps 250\n",
            "Episode: 54 Reward: -0.78 Average Reward: -0.3238000000000001 Steps 250\n",
            "Episode: 55 Reward: -0.74 Average Reward: -0.321 Steps 250\n",
            "Episode: 56 Reward: 0.47 Average Reward: -0.29180000000000006 Steps 54\n",
            "Episode: 57 Reward: -0.61 Average Reward: -0.28540000000000004 Steps 250\n",
            "Episode: 58 Reward: -0.78 Average Reward: -0.28500000000000003 Steps 250\n",
            "Episode: 59 Reward: 0.85 Average Reward: -0.2822 Steps 26\n",
            "Episode: 60 Reward: -0.62 Average Reward: -0.3022 Steps 250\n",
            "Episode: 61 Reward: -0.8300000000000001 Average Reward: -0.29460000000000003 Steps 250\n",
            "Episode: 62 Reward: 0.05999999999999994 Average Reward: -0.30660000000000004 Steps 229\n",
            "Episode: 63 Reward: 0.61 Average Reward: -0.28040000000000004 Steps 135\n",
            "Episode: 64 Reward: 0.42999999999999994 Average Reward: -0.25900000000000006 Steps 198\n",
            "Episode: 65 Reward: 0.47 Average Reward: -0.23500000000000004 Steps 189\n",
            "Episode: 66 Reward: -0.8200000000000001 Average Reward: -0.23460000000000003 Steps 250\n",
            "Episode: 67 Reward: -0.68 Average Reward: -0.2584000000000001 Steps 250\n",
            "Episode: 68 Reward: 0.5599999999999999 Average Reward: -0.23120000000000004 Steps 119\n",
            "Episode: 69 Reward: -0.52 Average Reward: -0.2548 Steps 250\n",
            "Episode: 70 Reward: 0.23999999999999988 Average Reward: -0.2576 Steps 153\n",
            "Episode: 71 Reward: -0.77 Average Reward: -0.28140000000000004 Steps 250\n",
            "Episode: 72 Reward: 0.85 Average Reward: -0.2770000000000001 Steps 62\n",
            "Episode: 73 Reward: 0.6300000000000001 Average Reward: -0.27 Steps 169\n",
            "Episode: 74 Reward: -0.7 Average Reward: -0.2922 Steps 250\n",
            "Episode: 75 Reward: 0.18999999999999995 Average Reward: -0.29940000000000005 Steps 182\n",
            "Episode: 76 Reward: -0.89 Average Reward: -0.32500000000000007 Steps 250\n",
            "Episode: 77 Reward: -0.79 Average Reward: -0.35900000000000004 Steps 250\n",
            "Episode: 78 Reward: -0.6100000000000001 Average Reward: -0.35359999999999997 Steps 250\n",
            "Episode: 79 Reward: 0.89 Average Reward: -0.31939999999999996 Steps 17\n",
            "Episode: 80 Reward: -0.75 Average Reward: -0.3502 Steps 250\n",
            "Episode: 81 Reward: 0.31999999999999995 Average Reward: -0.3526 Steps 159\n",
            "Episode: 82 Reward: 0.96 Average Reward: -0.3386 Steps 8\n",
            "Episode: 83 Reward: 0.37 Average Reward: -0.3152 Steps 179\n",
            "Episode: 84 Reward: -0.81 Average Reward: -0.31520000000000004 Steps 250\n",
            "Episode: 85 Reward: -0.77 Average Reward: -0.3148 Steps 250\n",
            "Episode: 86 Reward: -0.67 Average Reward: -0.3118 Steps 250\n",
            "Episode: 87 Reward: 0.7 Average Reward: -0.30140000000000006 Steps 52\n",
            "Episode: 88 Reward: 0.75 Average Reward: -0.273 Steps 88\n",
            "Episode: 89 Reward: -0.77 Average Reward: -0.2722 Steps 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on MiniHack-WoD-Easy-v0"
      ],
      "metadata": {
        "id": "sAeXmu_vn-LL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "env = gym.make(\"MiniHack-WoD-Easy-v0\", observation_keys=[\"glyphs\",\"pixel\",\"message\"],max_episode_steps=1000)\n",
        " \n",
        "WoD_rewards = []\n",
        "\n",
        "seeds = getSeeds(5)\n",
        "for seed in seeds:\n",
        "  act_critic_model = ActorCritic3(h_size=512, a_size=env.action_space.n)\n",
        "  policy, model_rewards = run(env=env,model = act_critic_model, seed = seed)\n",
        "  WoD_rewards .append(model_rewards)\n",
        "\n",
        "plot_results(\"MiniHack-WoD-Easy-v0\",WoD_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "7MAGLQjBoFzV",
        "outputId": "363a8a10-8751-4cdb-8991-80692658b1d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 Reward: -0.45000000000000007 Average Reward: -0.45000000000000007 Steps 50\n",
            "Episode: 1 Reward: -0.5 Average Reward: -0.47500000000000003 Steps 50\n",
            "Episode: 2 Reward: -0.4700000000000001 Average Reward: -0.4733333333333334 Steps 50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a823ad8fe05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mact_critic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_critic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mWoD_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e3c21d8b4f38>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(env, model, seed, verbose)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculateLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run on MiniHack-Quest-Hard-v0"
      ],
      "metadata": {
        "id": "vnIB1iQSpPSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Reward for exploring more of the map \n",
        "def maze_explore_reward(env, prev_obs, action, next_obs):\n",
        "    if (prev_obs[0] == 2359).sum() > (next_obs[0] == 2359).sum():\n",
        "        return 0.1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Z9oHLjlhe2hh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward Manager for including custom rewards\n",
        "from minihack import RewardManager\n",
        "\n",
        "reward_gen = RewardManager()\n",
        "\n",
        "\n",
        "reward_gen.add_eat_event(\"apple\", reward=1)\n",
        "\n",
        "# Custom Rewards for long corridors at top and bottom \n",
        "reward_gen.add_coordinate_event((3,27), reward = -5, terminal_required = False)\n",
        "reward_gen.add_coordinate_event((3,28), reward = -5, terminal_required = False)\n",
        "reward_gen.add_coordinate_event((3,29), reward = -5, terminal_required = False)\n",
        "\n",
        "reward_gen.add_coordinate_event((19,27), reward = -5, terminal_required = False)\n",
        "reward_gen.add_coordinate_event((19,28), reward = -5, terminal_required = False)\n",
        "reward_gen.add_coordinate_event((19,29), reward = -5, terminal_required = False)\n",
        "\n",
        "reward_gen.add_coordinate_event((11,27), reward = 100, terminal_required = False)# first door at end of maze\n",
        "\n",
        "#our rewards \n",
        "reward_gen.add_kill_event(\"minotaur\", reward=11)\n",
        "strings = list()\n",
        "strings.append(\"The door opens.\")\n",
        "reward_gen.add_message_event(strings, reward=1)\n",
        "######\n",
        "strings = list()\n",
        "strings.append(\"It's solid stone.\")\n",
        "reward_gen.add_message_event(strings, reward=-0.75)\n",
        "\n",
        "reward_gen.add_custom_reward_fn(maze_explore_reward)"
      ],
      "metadata": {
        "id": "u8Bkrgfae3pJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moves = tuple(nethack.CompassDirection)\n",
        "ACTIONS = moves + ( \n",
        "    nethack.CompassDirection.NW,\n",
        "    nethack.Command.PICKUP,\n",
        "    nethack.Command.APPLY,\n",
        "    nethack.Command.FIRE,\n",
        "    nethack.Command.RUSH,\n",
        "    nethack.Command.ZAP, \n",
        "    nethack.Command.PUTON,\n",
        "    nethack.Command.AUTOPICKUP,\n",
        "    nethack.Command.OPEN,\n",
        "    nethack.Command.KICK,\n",
        "    nethack.Command.READ,\n",
        "    nethack.Command.SEARCH,\n",
        "    nethack.Command.WEAR,\n",
        "    nethack.Command.WIELD,\n",
        "    nethack.Command.QUAFF\n",
        ")\n",
        "\n",
        "env = gym.make(\"MiniHack-Quest-Hard-v0\", reward_lose = -10, reward_win = 10, reward_manager = reward_gen, actions = ACTIONS)\n",
        "\n",
        "\n",
        "quest_hard_rewards =[]\n",
        "seeds = getSeeds(3)\n",
        "state = format_state(env.reset())\n",
        "for seed in seeds:\n",
        "  act_critic_model = ActorCritic3(h_size=512, a_size=env.action_space.n)\n",
        "  policy, model_rewards = run(env=env,model = act_critic_model, seed = seed )\n",
        "  quest_hard_rewards.append(model_rewards)\n",
        "\n",
        "plot_results(\"MiniHack-Quest-Hard-v0\",quest_hard_rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "j2NXEpUae7xa",
        "outputId": "522524b9-dbbd-4c3a-8ae9-6f0f899365a0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-62d58e1c48ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mact_critic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mact_critic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0mquest_hard_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-e3c21d8b4f38>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(env, model, seed, verbose)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m#COOKED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Take selected action, observe the reward received, the next state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-777d57714fe5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Pass the 2D glyphs input through our convolutional and pooling layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mglyphs_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglyphs_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Pass the message input through a fully connected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q7oQz9XGfHpa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}